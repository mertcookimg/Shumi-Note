{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLにおけるKLとエントロピー正則化\n",
    "\n",
    "強化学習アルゴリズムではKLやエントロピー正則化がよく出てきます。\n",
    "この正則化を導入すると、アルゴリズムにどのような違いが生じるでしょうか？\n",
    "\n",
    "今回は強化学習と正則化の理論について見ていきます。\n",
    "\n",
    "参考：\n",
    "\n",
    "* POLITEX\n",
    "* [Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning](https://arxiv.org/abs/2003.14089)\n",
    "* Theoretical Analysis\n",
    "* KL entropy minimax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLとエントロピーとルジャンドル変換\n",
    "\n",
    "参考\n",
    "* [Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning](https://arxiv.org/abs/2003.14089)\n",
    "* [Convex Optimization]()\n",
    "\n",
    "表記\n",
    "* $q$についての貪欲方策：$\\mathcal{G}(q) = \\argmax_{\\pi} \\langle q, \\pi \\rangle$\n",
    "* ベルマン期待作用素：$T_\\pi q = r + \\gamma P_{\\pi} q$\n",
    "\n",
    "これから先では、次のように、方策更新と方策評価のそれぞれについてKLやエントロピー正則化が入ったアルゴリズムを見ていきます。\n",
    "\n",
    "* KLとエントロピー正則化のもとでの貪欲方策：$\\mathcal{G}^{\\lambda, \\tau}_{\\mu}(q) = \\argmax_{\\pi} \\langle q, \\pi \\rangle - \\lambda \\mathrm{KL}(\\pi \\| \\mu) + \\tau \\mathcal{H}(\\pi)$\n",
    "* KLとエントロピー正則化のもとでのベルマン期待作用素：$T_{\\pi | \\mu}^{\\lambda, \\tau} q = r + \\gamma P (\\langle \\pi, q\\rangle - \\lambda \\mathrm{KL}(\\pi \\| \\mu) + \\tau \\mathcal{H}(\\pi))$\n",
    "\n",
    "簡単のため、行動空間が有限であり、期待値が厳密に計算できる場合を考えましょう。\n",
    "このとき、KLダイバージェンスやエントロピーは簡単に計算することができます。そのため、ベルマン期待作用素の計算は簡単に計算できます。では、正則化がある場合の貪欲方策はどうでしょうか？$\\argmax$の中にKLやエントロピーが入っているので、一見すると厄介な計算に見えます。\n",
    "\n",
    "実は、凸共役の性質（ルジャンドル変換）を上手く使うと、KLとエントロピーによって正則化された貪欲方策は解析解が求まります。\n",
    "\n",
    "---\n",
    "\n",
    "**ルジャンドル変換とエントロピー正則化**\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLとエントロピーとルジャンドル変換\n",
    "\n",
    "参考\n",
    "* [Leverage the Average: an Analysis of KL Regularization in Reinforcement Learning](https://arxiv.org/abs/2003.14089)\n",
    "\n",
    "表記\n",
    "* $q$についての貪欲方策：$\\mathcal{G}(q) = \\argmax_{\\pi} \\langle q, \\pi \\rangle$\n",
    "* ベルマン期待作用素：$T_\\pi q = r + \\gamma P_{\\pi} q$\n",
    "* KLとエントロピー正則化のもとでの貪欲方策：$\\mathcal{G}^{\\lambda, \\tau}_{\\mu}(q) = \\argmax_{\\pi} \\langle q, \\pi \\rangle - \\lambda \\mathrm{KL}(\\pi \\| \\mu) + \\tau \\mathcal{H}(\\pi)$\n",
    "* KLとエントロピー正則化のもとでのベルマン期待作用素：$T_{\\pi | \\mu}^{\\lambda, \\tau} q = r + \\gamma P (\\langle \\pi, q\\rangle - \\lambda \\mathrm{KL}(\\pi \\| \\mu) + \\tau \\mathcal{H}(\\pi))$\n",
    "\n",
    "---\n",
    "\n",
    "**Approximate Modified Policy Iteration**\n",
    "\n",
    "Approximate Modified Policy Iteration (AMPI) は動的計画法の漸近的な挙動が誤差にどれくらい影響されるかを解析するためのフレームワークです。\n",
    "[Approximate modified policy iteration and its application to the game of tetris](https://mohammadghavamzadeh.github.io/PUBLICATIONS/jmlr-ampi.pdf)などが参考になります。\n",
    "\n",
    "今回は「Q関数の更新にのみ」誤差が乗った形を考えます。方策側にも誤差が乗ると解析が難しくなります。\n",
    "AMPIは次の更新を繰り返します。\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\pi_{k+1} \\in \\mathcal{G}(q_k) \\\\\n",
    "q_{k+1} = (T_{\\pi_{k+1}})^m q_k + \\epsilon_{k+1}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* $m=1$のとき：**Approximate Value Iteration**と呼ばれます。\n",
    "* $m=\\infty$のとき：**Approximate Policy Iteration**と呼ばれます。\n",
    "* $\\epsilon_k$はなんらかの近似誤差です。\n",
    "  * 例えばDQNなら１ステップ回帰に伴う誤差になります。\n",
    "  * $m$ステップ回帰を回した場合、$m>1$に相当します。\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "**Mirror Descent MPI**\n",
    "\n",
    "TODO: 名前の由来やFrank wolfとの関連\n",
    "AMPIにKLとエントロピー正則化が乗ったアルゴリズムはMirror Descent MPI (MD-MPI) と呼ばれます。\n",
    "\n",
    "MD-MPIは次の更新を繰り返します。\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\pi_{k+1} \\in \\mathcal{G}^{\\lambda, \\tau}_{\\pi_k}(q_k) \\\\\n",
    "q_{k+1} = (T^{\\lambda, \\tau}_{\\pi_{k+1} | \\pi_{k}})^m q_k + \\epsilon_{k+1}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動的計画法での誤差頑健性　"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabularでの解析"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearの解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
