{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 統計的学習理論\n",
    "\n",
    "参考：\n",
    "* [統計的学習理論 (機械学習プロフェッショナルシリーズ)](https://amzn.asia/d/5OudVVi)\n",
    "\n",
    "入力空間から出力空間への関数のことを仮説と呼びます。\n",
    "統計的学習理論とは、学習アルゴリズムにより得られる仮説の予測精度を評価し、性能を向上させるための指針を与える理論的枠組みです。\n",
    "\n",
    "## 用語\n",
    "\n",
    "* 仮説$h$の予測損失：$R(h)=\\mathbb{E}_{(X, Y)\\sim D}[\\ell(h(X), Y)]$\n",
    "* 仮説$h$の経験損失：$\\hat{R}(h)=\\frac{1}{n}\\sum_{i=1}^n\\ell(h(X_i), Y_i)$\n",
    "\n",
    "経験損失は期待値を使って表すこともできます。データ数が$n$のとき、確率$1/n$で$(X_i, Y_i)$に値を取る確率変数を$(X, Y)$とします。この分布を$\\hat{D}$とすれば、\n",
    "\n",
    "$$\\hat{R}(h)=\\mathbb{E}_{(X, Y)\\sim \\hat{D}}[\\ell(h(X), Y)]$$\n",
    "\n",
    "と表すこともできます。\n",
    "\n",
    "各データ$(X_i, Y_i)$が同一の分布$D$に従うとき、経験損失の期待値は予測損失に一致します。実際、$n$個の観測データの同時分布を$D^n$とすると、\n",
    "\n",
    "$$\\mathbb{E}_{D^n}[\\hat{R}(h)]=\\mathbb{E}_{D^n}\\left[\\frac{1}{n}\\sum_{i=1}^n\\ell(h(X_i), Y_i)\\right] = R(h)$$\n",
    "\n",
    "が成り立ちます。また、観測データが独立に同一の分布$D$に従うなら、大数の法則から\n",
    "\n",
    "$\\lim_{n\\to\\infty} \\operatorname{Pr}_{D^n}(|\\hat{R}(h) - R(h)| > \\varepsilon) = 0$\n",
    "\n",
    "が成り立ちます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ベイズ規則とベイズ誤差\n",
    "\n",
    "損失関数$\\ell$を定めたとき、任意の可測関数$h: \\mathcal{X} \\to \\mathcal{Y}$のもとでの予測損失の下限\n",
    "\n",
    "$$\\inf_{h:\\text{可測}} R(h)$$\n",
    "\n",
    "を$\\ell$のもとでのベイズ誤差といいます。この下限を達成する仮説が存在する場合、その仮説をベイズルールといいます。\n",
    "さきほどの予測損失を振り返ると、タワールールより\n",
    "\n",
    "$$R(h)=\\mathbb{E}_{X}[\\mathbb{E}_Y[\\ell(h(X), Y)|X]]$$\n",
    "が成り立ちます。よって、各入力$X=x$における条件付き期待値\n",
    "\n",
    "$$\\mathbb{E}_Y[\\ell(h(x), Y)|x]=\\int_\\mathcal{Y} \\ell(h(x), y)dP(y|x)$$\n",
    "\n",
    "を最小にする仮説$h$を選ぶと、予測誤差が最小になります。\n",
    "\n",
    "---\n",
    "\n",
    "**例（回帰問題）**\n",
    "\n",
    "例えば$\\ell(\\hat{y}, y)=(\\hat{y}-y)^2$のとき、\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_Y[\\ell(h, Y)] = (h - \\mathbb{E}[Y])^2 + V[Y]\n",
    "$$\n",
    "このとき、ベイズ規則は$h(x) = \\mathbb{E}[Y|x]$によって与えられます。\n",
    "さらに、ベイズ誤差は\n",
    "\n",
    "$$\n",
    "R(h) = \\mathbb{E}_X[V[Y|X]] = \\mathbb{E}_X\\left[\\int(y - \\mathbb{E}[Y|X])^2dP(y|X)\\right]\n",
    "$$\n",
    "\n",
    "になります。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習アルゴリズムの性能評価\n",
    "\n",
    "**期待予測損失（リスク）による評価**\n",
    "\n",
    "アルゴリズムに対して、データ$S=\\{(X_1, Y_1), \\dots, (X_n, Y_n)\\}$から得られる仮説を$h_S$とします。\n",
    "特に観測データ$S$の分布$D^n$に関する予測損失の期待値を期待予測損失（リスク）とよび、\n",
    "$\\mathbb{E}_{S\\sim D^n}[R(h_S)]$\n",
    "で定義されます。\n",
    "\n",
    "期待予測損失の大小によって、学習アルゴリズムの平均的な性能を評価できます。\n",
    "\n",
    "**確率的な評価**\n",
    "\n",
    "ベイズ誤差を$R^*=\\inf_h R(h)$とします。このとき、\n",
    "\n",
    "$$\\operatorname{Pr}_{S\\sim D^n} (R(h_S) - R^* < \\varepsilon) > 1 - \\delta$$\n",
    "が成り立つとします。このとき、$varepsilon$の値と$\\delta$の値を調べることで性能を評価できます。\n",
    "\n",
    "マルコフの不等式から、\n",
    "\n",
    "$$\n",
    "\\operatorname{Pr}_{S\\sim D^n} (R(h_S) - R^* \\geq \\varepsilon) \\leq \\frac{\\mathbb{E}_{S\\sim D^n}[R(h_S)] - R^*}{\\varepsilon}\n",
    "$$\n",
    "が成り立ちます。\n",
    "\n",
    "この左辺のように、ベイズ誤差に近い予測損失を達成する仮説を求められるアルゴリズムは、**統計的一致性**をもつといいます：\n",
    "\n",
    "---\n",
    "\n",
    "**統計的一致性**\n",
    "\n",
    "任意の分布$D$と任意の$\\varepsilon > 0$について、\n",
    "\n",
    "$$\n",
    "\\lim_{n\\to \\infty} \\operatorname{Pr}_{S\\sim D^n} (R(h_S) \\leq R^* + \\varepsilon) = 0\n",
    "$$\n",
    "\n",
    "が成り立つとき、学習アルゴリズム$S\\mapsto h_S$は統計的一致性を持つといいます。\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有限な仮説集合の場合の学習\n",
    "\n",
    "有限な仮説集合$\\mathcal{H}=\\{h_1, \\cdots, h_T\\}$における二値判別問題を考えてみます。\n",
    "\n",
    "各仮説は入力空間$\\mathcal{X}$から二値ラベル$\\{+1, -1\\}$への関数です。また、データ$S$に対する学習アルゴリズムの出力を\n",
    "\n",
    "$$h_S = \\arg \\min_{h\\in \\mathcal{H}} \\hat{R}_{err}(h)$$\n",
    "\n",
    "とします（経験判別誤差を最小にしています）。分布$P$のもとでのベイズ規則は$h_0$としますが、$h_0$が$\\mathcal{H}$に含まれるとは限りません。\n",
    "さらに、$\\mathcal{H}$の中で予測判別誤差を最小にする仮説を$h_\\mathcal{H}$とします。\n",
    "\n",
    "このとき、定義から\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&R_{err}(h_0) \\leq R_{err}(h_\\mathcal{H}) \\leq R_{err}(h_S)\\\\\n",
    "&\\hat{R}_{err}(h_S) \\leq R_{err}(h_\\mathcal{H})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立ちます。これを使って、ベイズ規則との予測判別誤差の差を次のようにバウンドします\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "R_{err}(h_S) - R_{err}(h_0) &= \n",
    "R_{err}(h_S) - \\hat{R}_{err}(h_S)  + \\hat{R}_{err}(h_S) - \n",
    "R_{err}(h_\\mathcal{H}) + R_{err}(h_\\mathcal{H}) - R_{err}(h_0) \\\\\n",
    "&\\leq \n",
    "R_{err}(h_S) - \\hat{R}_{err}(h_S)  + \\hat{R}_{err}(h_\\mathcal{H}) - \n",
    "R_{err}(h_\\mathcal{H}) + R_{err}(h_\\mathcal{H}) - R_{err}(h_0) \\\\\n",
    "&\\leq \n",
    "2\\max_{h\\in \\mathcal{H}} |\\hat{R}_{err}(h) - R_{err}(h)| +  R_{err}(h_\\mathcal{H}) - R_{err}(h_0) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この第一項をHoeffdingの不等式（教科書参照）を使ってバウンドします。\n",
    "\n",
    "すると、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\operatorname{Pr}\\left(2\\max_{h\\in \\mathcal{H}}|\\hat{R}_{err}(h) - R_{err}(h)| \\geq \\varepsilon \\right)\n",
    "&\\leq \\sum_{h\\in \\mathcal{H}} \\operatorname{Pr}\\left(2|\\hat{R}_{err}(h) - R_{err}(h)| \\geq \\varepsilon \\right)\\\\\n",
    "&\\leq \\sum_{h\\in \\mathcal{H}} 2\\exp(-2n(\\varepsilon / 2)^2)\\\\\n",
    "&= 2 |\\mathcal{H}|\\exp(-2n(\\varepsilon / 2)^2)\n",
    "\\end{aligned}\n",
    "$$\n",
    "が成り立ちます。\n",
    "変形すると、確率$1-\\delta$以上で\n",
    "\n",
    "$$\n",
    "2 |\\mathcal{H}|\\exp(-2n(\\varepsilon / 2)^2) \\leq \\sqrt{\\frac{2}{n}\\log{\\frac{2|\\mathcal{H}|}{\\delta}}}\n",
    "$$\n",
    "\n",
    "が成り立ちます。\n",
    "よって、\n",
    "\n",
    "$$\n",
    "R_{err}(h_S) - R_{err}(h_0) \\leq \\sqrt{\\frac{2}{n}\\log{\\frac{2|\\mathcal{H}|}{\\delta}}}+  R_{err}(h_\\mathcal{H}) - R_{err}(h_0) \n",
    "$$\n",
    "\n",
    "です。\n",
    "\n",
    "ここで、確率オーダーの尺度を導入して評価してみます。\n",
    "\n",
    "---\n",
    "\n",
    "**確率オーダー**\n",
    "\n",
    "確率変数列$\\{Z_n\\}_{n\\in \\mathbb{N}}$の確率オーダーが$\\mathcal{O}_P(r_n)$であるとは、\n",
    "\n",
    "$$\n",
    "\\lim_{z\\to \\infty} \\lim \\sup_{n\\to \\infty} \\operatorname{Pr}(|Z_n|/r_n > z) = 0\n",
    "$$\n",
    "\n",
    "であることを意味します。\n",
    "\n",
    "---\n",
    "\n",
    "すると、とくに$h_0 \\in \\mathcal{H}$なら、\n",
    "\n",
    "$$\n",
    "R_{err}(h_S) = R_{err}(h_0) + \\mathcal{O}_P\\left(\\sqrt{\\frac{\\log{|\\mathcal{H}|}}{n}}\\right)\n",
    "$$\n",
    "\n",
    "と評価できます。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 近似誤差と推定誤差\n",
    "\n",
    "近似誤差（bias）と推定誤差（variance）を導入します。\n",
    "\n",
    "* 近似誤差：$\\operatorname{bias}_\\mathcal{H}=R_{err}(h_\\mathcal{H}) - R_{err}(h_0)$\n",
    "* 推定誤差：$\\operatorname{var}_\\mathcal{H}=\\sqrt{\\frac{2}{n}\\log{\\frac{2|\\mathcal{H}|}{\\delta}}}$\n",
    "\n",
    "このとき、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
