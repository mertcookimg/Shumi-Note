{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最尤推定の統計的な性質\n",
    "\n",
    "## 漸近的な性質\n",
    "\n",
    "まずは最尤推定の漸近的な収束速度を見てみましょう。\n",
    "\n",
    "参考と表記\n",
    "* [Empirical Processes in M-Estimation](https://www.amazon.co.jp/-/en/Sara-van-Geer/dp/0521123259)：６ページのExample 1.2\n",
    "\n",
    "**表記**\n",
    "\n",
    "* Xの密度：$p_{\\theta_0}(x)$\n",
    "* 最尤推定：$\\hat{\\theta}_n$\n",
    "    * $\\sum_{i=1}^n\\log p_{\\theta} (X_i)$を最大化させる$\\theta\\in \\Theta$\n",
    "    * ($\\hat{\\theta}_n$はempirical estimator)：このとき$\\sum_{i=1}^n\\log \\frac{p_{\\theta_0} (X_i)}{p_{\\hat{\\theta}_n} (X_i)} \\leq 0$が成立\n",
    "* KLダイバージェンス：$K(p_\\theta, p_{\\theta_0})=\\int \\left( \\log\\frac{p_{\\theta_0}}{{p_\\theta}}p_{\\theta_0}\\right)d\\mu$\n",
    "    * $g_\\theta = \\log\\frac{p_{\\theta_0}}{{p_\\theta}}$とすると\n",
    "    * $K(p_\\theta, p_{\\theta_0})=E [g_\\theta(X)]$\n",
    "\n",
    "まず、$\\theta_0$はpopulation estimatorなので、\n",
    "\n",
    "$$E \\log \\frac{p_{\\theta_0} (X_i)}{p_{\\theta} (X_i)} \\geq 0$$\n",
    "\n",
    "も任意の$\\theta$で成立。これはKLダイバージェンスであることに注意（$E[g_\\theta(X)]$と同じ）。\n",
    "\n",
    "最尤推定の性質と合わせると、\n",
    "\n",
    "$$0 \\geq \\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i)=\\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i) - K(p_{\\hat{\\theta}_n}, p_{\\theta_0}) + K(p_{\\hat{\\theta}_n}, p_{\\theta_0})$$\n",
    "\n",
    "であり、\n",
    "\n",
    "$$K(p_{\\hat{\\theta}_n}, p_{\\theta_0}) \\leq \\left|\\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right|$$\n",
    "\n",
    "が成立してます。\n",
    "ここで、サンプルによる尤度で２つの分布のダイバージェンスをバウンドしているのがコツです。このように、ヘリンジャー距離やKLダイバージェンスはサンプルの尤度で上から抑えることができます。\n",
    "\n",
    "よって、大数の方策と合わせると、\n",
    "\n",
    "$$\\left|\\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right| \\to 0 \\ \\text{a.s.,}$$\n",
    "\n",
    "が成立しています。よって、$K(p_{\\hat{\\theta}_n}, p_{\\theta_0}) \\to 0$ a.s.,も成り立っています。\n",
    "KLダイバージェンスは距離ではないですが、次の式を使えば、Hellinger距離についても一致性が保証されていることもわかります。\n",
    "Hellinger距離は\n",
    "\n",
    "$$\n",
    "h(p, q):=\\left(\\frac{1}{2}\\int (\\sqrt{p}-\\sqrt{q})^2 d \\mu\\right)^{1/2} .\n",
    "$$\n",
    "\n",
    "で定義されます。\n",
    "このとき、\n",
    "\n",
    "$$\n",
    "h^2(p, q) \\leq \\frac{1}{2} K(p, q)\n",
    "$$\n",
    "\n",
    "が成立します（証明は[Empirical Processes in M-Estimation](https://www.amazon.co.jp/-/en/Sara-van-Geer/dp/0521123259)の8ページ。）\n",
    "ちなみに全変動距離はHellinger距離でバウンドできます（[Introduction to Nonparametric Estimation](https://link.springer.com/book/10.1007/b13794)など）。\n",
    "\n",
    "また、中心極限定理から、\n",
    "\n",
    "$$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n} \\left(g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right) \\rightsquigarrow N(0, V(g_{\\theta_0}(X)))$$\n",
    "\n",
    "なので、\n",
    "\n",
    "$$\\left|\\frac{1}{{n}}\\sum_{i=1}^{n} \\left(g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right) \\right| = O_P(n^{-1/2})$$\n",
    "\n",
    "が成り立ちます。よって、$h(p_{\\hat{\\theta}_n}, p_{\\theta_0})=O_P(n^{-1/4})$であり、Hellinger距離は高速に収束することがわかります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最尤推定のPACバウンド\n",
    "\n",
    "続いてPACバウンドを導出してみます。\n",
    "\n",
    "参考：\n",
    "* [Empirical Processes in M-Estimation](https://www.amazon.co.jp/-/en/Sara-van-Geer/dp/0521123259)：７章\n",
    "\n",
    "表記：\n",
    "* 分布$P$に従う独立した確率変数$X_1, \\dots, X_n, \\dots$\n",
    "* $p_0=\\frac{dP}{d\\mu} \\in \\mathcal{P}$を真の確率密度関数とします（$\\mu$についてのラドン・ニコディム微分ってやつ。$\\mu$はルベーグ測度を考えるとわかりやすいかも。）\n",
    "* 最尤推定：$p_n = \\arg \\max_{p\\in \\mathcal{P}} \\sum_{i=1}^n\\log p (X_i)$\n",
    "* ヘリンジャー距離：$h(p_1, p_2)=\\left(\\frac{1}{2}\\int(\\sqrt{p_1}-\\sqrt{p_2})^2 d\\mu\\right)^{1/2}$\n",
    "\n",
    "以下では$h(\\hat{p}_n, p_0)$の収束レートを解析します。\n",
    "特に以下のエントロピーについての表記を利用します：\n",
    "\n",
    "* 平方根の密度関数の集合：$\\mathcal{P}^{1/2} = \\{\\sqrt{p}: p \\in \\mathcal{P}\\}$\n",
    "* $\\sqrt{p_0}$の０でない部分で割った平方根の密度関数の集合：$\\frac{\\mathcal{P}^{1/2}}{\\sqrt{p_0}} = \\{\\frac{\\sqrt{p}}{\\sqrt{p_0}}{\\bf 1}\\{p_0 > 0\\}: p \\in \\mathcal{P}\\}$\n",
    "* 真の$p_0$との平均：$\\bar{p}=\\frac{p+p_0}{2}$\n",
    "* $\\bar{\\mathcal{P}}=\\{\\bar{p}:p\\in \\mathcal{P}\\}$\n",
    "* $\\bar{\\mathcal{P}}^{1/2}=\\{\\bar{p}^{1/2}:\\bar{p}\\in \\bar{\\mathcal{P}}\\}$\n",
    "* $g_p = \\frac{1}{2} \\log\\frac{\\bar{p}}{p_0}{\\bf 1}\\{p_0 > 0\\}$\n",
    "* $\\mathcal{G}=\\{g_p : p \\in \\mathcal{P}\\}$\n",
    "* $p_0$が０でない部分の測度：$d\\mu_0 = {\\bf 1}\\{p_0 > 0\\} d\\mu$\n",
    "* $p_0$が０でない部分のヘリンジャー距離：$h_0(p_1, p_2)=\\left(\\frac{1}{2}\\int(\\sqrt{p_1}-\\sqrt{p_2})^2 d\\mu_0\\right)^{1/2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
