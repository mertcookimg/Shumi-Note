{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最尤推定の統計的な性質\n",
    "\n",
    "参考と表記\n",
    "* [Empirical Processes in M-Estimation](https://www.amazon.co.jp/-/en/Sara-van-Geer/dp/0521123259)\n",
    "\n",
    "\n",
    "**表記**\n",
    "\n",
    "* Xの密度：$p_{\\theta_0}(x)$\n",
    "* 最尤推定：$\\hat{\\theta}_n$\n",
    "    * $\\sum_{i=1}^n\\log p_{\\theta} (X_i)$を最大化させる$\\theta\\in \\Theta$\n",
    "    * ($\\hat{\\theta}_n$はempirical estimator)：このとき$\\sum_{i=1}^n\\log \\frac{p_{\\theta_0} (X_i)}{p_{\\hat{\\theta}_n} (X_i)} \\leq 0$が成立\n",
    "* KLダイバージェンス：$K(p_\\theta, p_{\\theta_0})=\\int \\left( \\log\\frac{p_{\\theta_0}}{{p_\\theta}}p_{\\theta_0}\\right)d\\mu$\n",
    "    * $g_\\theta = \\log\\frac{p_{\\theta_0}}{{p_\\theta}}$とすると\n",
    "    * $K(p_\\theta, p_{\\theta_0})=E [g_\\theta(X)]$\n",
    "\n",
    "まず、$\\theta_0$はpopulation estimatorなので、\n",
    "\n",
    "$$E \\log \\frac{p_{\\theta_0} (X_i)}{p_{\\theta} (X_i)} \\geq 0$$\n",
    "\n",
    "も任意の$\\theta$で成立。これはKLダイバージェンスであることに注意（$E[g_\\theta(X)]$と同じ）。\n",
    "\n",
    "最尤推定の性質と合わせると、\n",
    "\n",
    "$$0 \\geq \\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i)=\\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i) - K(p_{\\hat{\\theta}_n}, p_{\\theta_0}) + K(p_{\\hat{\\theta}_n}, p_{\\theta_0})$$\n",
    "\n",
    "であり、\n",
    "\n",
    "$$K(p_{\\hat{\\theta}_n}, p_{\\theta_0}) \\leq \\left|\\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right|$$\n",
    "\n",
    "が成立してます。よって、大数の方策と合わせると、\n",
    "\n",
    "$$\\left|\\frac{1}{n}\\sum_{i=1}^{n}g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right| \\to 0 \\ \\text{a.s.,}$$\n",
    "\n",
    "が成立しています。よって、$K(p_{\\hat{\\theta}_n}, p_{\\theta_0}) \\to 0$ a.s.,も成り立っています。\n",
    "KLダイバージェンスは距離ではないですが、次の式を使えば、Hellinger距離についても一致性が保証されていることもわかります。\n",
    "Hellinger距離は\n",
    "\n",
    "$$\n",
    "h(p, q):=\\left(\\frac{1}{2}\\int (\\sqrt{p}-\\sqrt{q})^2 d \\mu\\right)^{1/2} .\n",
    "$$\n",
    "\n",
    "で定義されます。\n",
    "このとき、\n",
    "\n",
    "$$\n",
    "h^2(p, q) \\leq \\frac{1}{2} K(p, q)\n",
    "$$\n",
    "\n",
    "が成立します（証明は[Empirical Processes in M-Estimation](https://www.amazon.co.jp/-/en/Sara-van-Geer/dp/0521123259)の8ページ。）\n",
    "ちなみに全変動距離はHellinger距離でバウンドできます（[Introduction to Nonparametric Estimation](https://link.springer.com/book/10.1007/b13794)など）。\n",
    "\n",
    "また、中心極限定理から、\n",
    "\n",
    "$$\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n} \\left(g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right) \\rightsquigarrow N(0, V(g_{\\theta_0}(X)))$$\n",
    "\n",
    "なので、\n",
    "\n",
    "$$\\left|\\frac{1}{{n}}\\sum_{i=1}^{n} \\left(g_{\\hat{\\theta}_n}(X_i)- K(p_{\\hat{\\theta}_n}, p_{\\theta_0})\\right) \\right| = O_P(n^{-1/2})$$\n",
    "\n",
    "が成り立ちます。よって、$h(p_{\\hat{\\theta}_n}, p_{\\theta_0})=O_P(n^{-1/4})$であり、Hellinger距離は高速に収束することがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
