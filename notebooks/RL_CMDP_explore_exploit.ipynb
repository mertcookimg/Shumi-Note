{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 制約付きMDPでの探索と活用\n",
    "\n",
    "参考\n",
    "\n",
    "* [Exploration-Exploitation in Constrained MDPs](https://arxiv.org/abs/2003.02189)\n",
    "\n",
    "強化学習では累積報酬和を最大化するような方策を学習することが目標ですが、実際のアプリケーションでは方策の挙動に対して何らかの制約が求められる場合があります。\n",
    "制約付きMDP（CMDP）では制約を満たしながらの累積報酬和の最大化が求められるわけですが、この状況での探索はどうすればよいでしょうか？\n",
    "\n",
    "\n",
    "表記\n",
    "\n",
    "* 有限MDP: $\\mathcal{M}=\\left(\\mathcal{S}, \\mathcal{A}, c, p, s_1, H\\right)$\n",
    "    1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "    2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "    3. 非定常なコスト関数（元論文では確率変数ですが、ややこしいので決定的にします）: $c_h(s, a)$\n",
    "    4. 非定常遷移確率: $p_h(s'|s, a)$\n",
    "    5. 初期状態: $s_1$\n",
    "    6. ホライゾン: $H$\n",
    "    7. すべての状態行動の中で最大の次状態への遷移の数: $\\mathcal{N}:=\\max _{s, a, h}\\left|\\left\\{s^{\\prime}: p_h\\left(s^{\\prime} \\mid s, a\\right)>0\\right\\}\\right|$\n",
    "* 占有率：$q_h^\\pi(s, a ; p):=\\mathbb{E}\\left[\\mathbb{1}\\left\\{s_h=s, a_h=a\\right\\} \\mid s_1=s_1, p, \\pi\\right]=\\operatorname{Pr}\\left\\{s_h=s, a_h=a \\mid s_1=s_1, p, \\pi\\right\\}$\n",
    "* 価値関数：$V_1^\\pi\\left(s_1 ; p, c\\right)=\\sum_{h, s, a} q_h^\\pi(s, a ; p) c_h(s, a):=c^T q^\\pi(p)$\n",
    "\n",
    "これは次の式で導出可能\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\mathbb{E}\\left[\\sum_{h=1}^H c_h\\left(s_h, a_h\\right) \\mid s_1=s_1, \\pi, p\\right]=\\sum_{h=1}^H \\mathbb{E}\\left[c_h\\left(s_h, a_h\\right) \\mid s_1=s_1, \\pi, p\\right] \\\\\n",
    "& =\\sum_{h=1}^H \\sum_{s, a} c_h(s, a) \\operatorname{Pr}\\left\\{s_h=s, a_h=a \\mid s_1=s_1, p, \\pi\\right\\} \\\\\n",
    "& \\sum_{h=1}^H \\sum_{s, a} c_h(s, a) q_h^\\pi(s, a ; p)=c^T q^\\pi(p),\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有限ホライゾンConstrained MDP\n",
    "\n",
    "* $\\{d_i, \\alpha_i\\}_{i=1}^I$：$I$個の制約\n",
    "    * $d_i \\in \\mathbb{R}^{SAH}$\n",
    "    * $\\alpha_i \\in [0, H]$\n",
    "    * $i$番目の制約（元論文では確率変数ですが、ややこしいので決定的にします）$d_{i, h}(s, a)$\n",
    "    * $V_h^\\pi\\left(s ; p, d_i\\right):=\\mathbb{E}\\left[\\sum_{h^{\\prime}=h}^H d_{i, h^{\\prime}}\\left(s_{h^{\\prime}}, a_{h^{\\prime}}\\right) \\mid s_h=s, p, \\pi\\right]$.\n",
    "\n",
    "CMDPの目的は次の最適方策の導出です。\n",
    "\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\pi^{\\star} \\in \\underset{\\pi \\in \\Pi^{\\mathrm{MR}}}{\\arg \\min } c^T q^\\pi(p) \\\\\n",
    "\\text { s.t. } D q^\\pi(p) \\leq \\alpha\n",
    "\\end{gathered}\n",
    "$$\n",
    "\n",
    "ここで、\n",
    "$$\n",
    "D=\\left[\\begin{array}{c}\n",
    "d_1^T \\\\\n",
    "\\vdots \\\\\n",
    "d_I^T\n",
    "\\end{array}\\right], \\quad \\alpha=\\left[\\begin{array}{c}\n",
    "\\alpha_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\alpha_I\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "以下では最適方策が方策集合の中にあることを仮定します（Feasibility）。\n",
    "\n",
    "また、最適方策は決定的でない可能性があることに注意しましょう。制約のせいで、通常のMDPで成り立つ性質がCMDPでも成り立つかは自明ではありません。\n",
    "\n",
    "CMDPの性能を測るために、次のリグレットを考えます。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\operatorname{Reg}_{+}(K ; c)=\\sum_{k=1}^K\\left[V_1^{\\pi_k}\\left(s_1 ; p, c\\right)-V_1^{\\star}\\left(s_1\\right)\\right]_{+} \\\\\n",
    "& \\operatorname{Reg}_{+}(K ; d)=\\max _{i \\in[I]} \\sum_{k=1}^K\\left[V_1^{\\pi_k}\\left(s_1 ; p, d_i\\right)-\\alpha_i\\right]_{+},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "こっちのリグレットを考えると大変なので、次のリグレットを考えることもあります。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\operatorname{Reg}(K ; c)=\\sum_{k=1} V_1^{\\pi_k}\\left(s_1 ; p, c\\right)-V_1^{\\star}\\left(s_1\\right) \\\\\n",
    "& \\operatorname{Reg}(K ; d)=\\max _{i \\in[I]}\\left[\\sum_{k=1}^K V_1^{\\pi_k}\\left(s_1 ; p, d_i\\right)-\\alpha_i\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "これは上で定義した$\\operatorname{Reg}_+$のほうが強い制約であることに注意しましょう。下のリグレットでは、負と正のviolation同士が打ち消し合うことがあります。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMDPでの線型計画法\n",
    "\n",
    "まず、任意の方策について占有率は次の方程式を満たします。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_a q_h^\\pi(s, a) & =\\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) q_{h-1}^\\pi\\left(s^{\\prime}, a^{\\prime}\\right) & & \\forall s \\in \\mathcal{S} \\\\\n",
    "q_h^\\pi(s, a) & \\geq 0 & & \\forall s, a\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "この方程式を満たす$q^\\pi$は確率測度であることに注意しましょう。\n",
    "確率測度全体の集合を$\\Delta^\\mu(\\mathcal{M})$とします。（これは凸集合になります。６ページの参考文献参照。）\n",
    "\n",
    "線型計画法の解で得られた占有率を使って方策を表すことができます。\n",
    "\n",
    "$$\n",
    "\\pi_h^q(a \\mid s)=\\frac{q_h(s, a)}{\\sum_b q_h(s, b)}, \\quad \\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "$$\n",
    "\n",
    "これを使うと、CMDPの問題は次の問題と等価になります。\n",
    "\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "& \\min _q \\sum_{s, a, h} q_h(s, a) c_h(s, a) & \\\\\n",
    "\\text { s.t. } & \\sum_{s, a, h} q_h(s, a) d_{i, h}(s, a) \\leq \\alpha_i & \\forall i \\in[I]\\\\\n",
    "& \\sum_a q_h(s, a)=\\sum_{s^{\\prime}, a^{\\prime}} p_{h-1}\\left(s \\mid s^{\\prime}, a^{\\prime}\\right) q_{h-1}\\left(s^{\\prime}, a^{\\prime}\\right) & \\forall h \\in[H] \\backslash\\{1\\}\\\\\n",
    "& \\sum_a q_1(s, a)=\\mu(s) & \\forall s \\in \\mathcal{S} \\\\\n",
    "& q_h(s, a) \\geq 0 & \\forall(s, a, h) \\in \\mathcal{S} \\times \\mathcal{A} \\times[H]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**その他の表記**\n",
    "\n",
    "任意の確率変数$X$について、\n",
    "\n",
    "$$\\mathbb{E}\\left[X\\left(s_h^k, a_h^k\\right) \\mid \\mathcal{F}_{k-1}\\right]=\\sum_{s, a} q_h^{\\pi_k}(s, a ; p) X(s, a)$$\n",
    "\n",
    "が成り立つことに注意しましょう。\n",
    "\n",
    "* $\\widetilde{O}(X)$は$X$に対数多項式的に依存する量とします\n",
    "* $\\lesssim$は対数多項式を無視した$\\leq$とします。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMDPでのUCRLアルゴリズム（OPTCMDP）\n",
    "\n",
    "まずはCMDP版のUCRL2アルゴリズムであるOptCMDPを見てみましょう。\n",
    "\n",
    "* $k$エピソード以前に訪れた状態行動の数：$n_h^{k-1}(s, a)=\\sum_{k^{\\prime}=1}^{k-1} \\mathbb{1}\\left(s_h^{k^{\\prime}}=s, a_h^{k^{\\prime}}=a\\right)$\n",
    "* これを使った経験平均値\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right) & =\\frac{\\sum_{k^{\\prime}=1}^{k-1} \\mathbb{1}\\left(s_h^{k^{\\prime}}=s, a_h^{k^{\\prime}}=a, s_{h+1}^{k^{\\prime}}=s^{\\prime}\\right)}{n_h^{k-1}(s, a) \\vee 1} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "まず、OptCMDPはUCRLみたいに信頼集合を構築します。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "B_{h, k}^p(s, a) & =\\left\\{\\widetilde{p}(\\cdot \\mid s, a) \\in \\Delta_S: \\forall s^{\\prime} \\in \\mathcal{S},\\left|\\widetilde{p}(\\cdot \\mid s, a)-\\bar{p}_h^{k-1}(\\cdot \\mid s, a)\\right| \\leq \\beta_{h, k}^p\\left(s, a, s^{\\prime}\\right)\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "ここで、その範囲$\\beta$は\n",
    "\n",
    "$$\\beta_{h, k}^p\\left(s, a, s^{\\prime}\\right):=2 \\sqrt{\\frac{\\operatorname{Var}\\left(\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right) L_\\delta^p}{n_h^{k-1}(s, a) \\vee 1}}+\\frac{14 / 3 L_\\delta^p}{n_h^{k-1}(s, a) \\vee 1}$$\n",
    "\n",
    "によって構築します。\n",
    "ここで、$\\operatorname{Var}\\left(\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right)=\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right) \\cdot\\left(1-\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right)$です。（Bernsteinを使ったUCRLでも似たような項が出てきます。）\n",
    "また、$L_\\delta^p=\\ln \\left(\\frac{6 S A H K}{\\delta}\\right)$です。\n",
    "\n",
    "この信頼集合を使って、\n",
    "$$\n",
    "\\mathcal{M}_k=\\left\\{M=(\\mathcal{S}, \\mathcal{A}, c, d, \\widetilde{p}): \\widetilde{p}_h(\\cdot \\mid s, a) \\in B_{h, k}^p(s, a)\\right\\}\n",
    "$$\n",
    "\n",
    "を定義します。\n",
    "信頼集合が定義されたら、次の最適化問題を解くことで方策を更新します。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{\\widetilde{p} \\in B_k^p, \\pi \\in \\Pi^{\\mathrm{MR}}} & \\sum_{h, s, a} {c}_h(s, a) q_h^\\pi(s, a ; \\widetilde{p}) \\\\\n",
    "\\text { s.t. } & \\sum_{h, s, a} {d}_{i, h}(s, a) q_h^\\pi(s, a ; \\widetilde{p}) \\leq \\alpha_i, \\quad \\forall i \\in[H]\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この問題で得られる方策を$\\pi_k$とし、得られるMDPを\n",
    "\n",
    "$$\n",
    "\\widetilde{M}_k=\\left(\\mathcal{S}, \\mathcal{A}, {c}_k, {d}_k, \\widetilde{p}_k\\right)\n",
    "$$\n",
    "\n",
    "とします。このとき、方策は\n",
    "\n",
    "$$\n",
    "V_1^{\\pi_k}\\left(s_1 ; {c}_k, {p}_k\\right):={c}_k^{\\top} q^{\\pi_k}\\left(\\widetilde{p}_k\\right) \\leq c^{\\top} q^{\\pi^{\\star}}(p):=V_1^{\\star}\\left(s_1 ; c, p\\right)\n",
    "$$\n",
    "\n",
    "の意味で楽観的になっています。\n",
    "\n",
    "これを証明しましょう。\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ１：Good eventの定義\n",
    "\n",
    "まず、Failure eventを定義します。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& F_k^p=\\left\\{\\exists s, a, s^{\\prime}, h:\\left|p_h\\left(s^{\\prime} \\mid s, a\\right)-\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right| \\geq \\beta_{h, k}^p\\left(s, a, s^{\\prime}\\right)\\right\\} \\\\\n",
    "& F_k^N=\\left\\{\\exists s, a, h: n_h^{k-1}(s, a) \\leq \\frac{1}{2} \\sum_{j<k} q_h^{\\pi_k}(s, a \\mid p)-H \\ln \\frac{S A H}{\\delta^{\\prime}}\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "また、$F^P=\\bigcup_{k=1}^K F_k^p$とします。\n",
    "つまり、\n",
    "\n",
    "* $F^p_k$：$k$エピソード目で、遷移確率が推定値から大幅にずれているイベント。証明は[Learning Adversarial MDPs with Bandit Feedback and Unknown Transition](https://arxiv.org/abs/1912.01192)参照。\n",
    "* $F^p$：$K$エピソード目までに一度でも推定値が大幅にずれているイベント。\n",
    "* $F^N_k$：全然訪れていない$(s, a)$があるイベント。証明は[Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning](https://arxiv.org/abs/1703.07710)参照。\n",
    "\n",
    "このとき、\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\operatorname{Pr}\\left\\{\\left|p_h\\left(s^{\\prime} \\mid s, a\\right)-\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right| \\geq \\epsilon_1\\right\\} \\leq \\delta^{\\prime \\prime}\\\\\n",
    "&\\epsilon_1=\\sqrt{\\frac{2 \\operatorname{Var}\\left(\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right) \\ln \\left(\\frac{2}{\\delta^{\\prime \\prime}}\\right)}{n_h^{k-1}(s, a) \\vee 1}}+\\frac{7 \\ln \\left(\\frac{2}{\\delta^{\\prime \\prime}}\\right)}{3\\left(n_h^{k-1}(s, a)-1\\right) \\vee 1} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "が成立します。また、\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\operatorname{Pr}\\left\\{\\left|p_h\\left(s^{\\prime} \\mid s, a\\right)-\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right| \\geq \\epsilon_2\\right\\} \\leq \\delta^{\\prime \\prime}\\\\\n",
    "&\\epsilon_2=\\sqrt{\\frac{2 \\operatorname{Var}\\left(\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)\\right) \\ln \\left(\\frac{2}{\\delta^{\\prime \\prime}}\\right)}{n_h^{k-1}(s, a) \\vee 1}}+\\frac{7 \\ln \\left(\\frac{2}{\\delta^{\\prime \\prime}}\\right)}{3\\left(n_h^{k-1}(s, a)-1 \\vee 1\\right)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "も成り立ちます。$\\epsilon_1 \\leq \\epsilon_2$なので。\n",
    "\n",
    "$\\delta^{\\prime \\prime}=\\frac{\\delta^{\\prime}}{(S A H K)^2}$としてUnion Boundをとれば、$\\operatorname{Pr}\\left\\{F^P\\right\\} \\leq \\delta^{\\prime}$が成り立ちます。\n",
    "\n",
    "\n",
    "以上のFailure Eventに対して、\n",
    "\n",
    "$\\bar{G}=F^p \\bigcup F^N$としてGood eventを定義します。\n",
    "このGood Eventが成り立っているとき、\n",
    "\n",
    "$$\n",
    "\\left|\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)-p_h\\left(s^{\\prime} \\mid s, a\\right)\\right|=C_1 \\sqrt{\\frac{p_h\\left(s^{\\prime} \\mid s, a\\right) L_{\\delta, p}}{n_h^k(s, a) \\vee 1}}+\\frac{C_2 L_{\\delta, p}}{n_h^k(s, a) \\vee 1}\n",
    "$$\n",
    "\n",
    "となる定数$C_1, C_2 > 0$が存在します。ここで$L_{\\delta, p}=\\ln \\left(\\frac{6 S A H K}{\\delta}\\right)$です。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ２：最適方策が$k$エピソード目で実行可能であることの証明\n",
    "\n",
    "まず、\n",
    "$$\n",
    "\\begin{gathered}\n",
    "\\pi^{\\star} \\in \\underset{\\pi \\in \\Pi^{\\mathrm{MR}}}{\\arg \\min } c^T q^\\pi(p) \\\\\n",
    "\\text { s.t. } D q^\\pi(p) \\leq \\alpha\n",
    "\\end{gathered}\n",
    "$$\n",
    "と\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{\\widetilde{p} \\in B_k^p, \\pi \\in \\Pi^{\\mathrm{MR}}} & \\sum_{h, s, a} {c}_h(s, a) q_h^\\pi(s, a ; \\widetilde{p}) \\\\\n",
    "\\text { s.t. } & \\sum_{h, s, a} {d}_{i, h}(s, a) q_h^\\pi(s, a ; \\widetilde{p}) \\leq \\alpha_i, \\quad \\forall i \\in[H]\n",
    "\\end{aligned}\n",
    "$$\n",
    "であることを思い出しましょう。また、Good eventが成立しているとき、真のモデル$p$は$B^p_k$の中に入ってることに注意しましょう。\n",
    "すると、明らかに$\\pi^*$は任意の$k$について実行可能です。\n",
    "\n",
    "（元論文の証明はちょっとよくわかんなかったです。$B$の中のすべての$p'$について制約を満たすようにすると必ずしも実行可能にならなさそうです。）\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ３：楽観的な方策の証明\n",
    "\n",
    "* $\\Pi_D=\\left\\{\\pi: D q^\\pi(p) \\leq \\alpha\\right\\}$を制約を満たす方策の集合\n",
    "* $\\Pi_D^k=\\left\\{\\pi: {D} q^\\pi\\left(p^{\\prime}\\right) \\leq \\alpha,\\; \\exists p^{\\prime} \\in B_k^p\\right\\}$ を何らかの$p'$に対して制約を満たすような方策の集合（$\\pi^\\star \\in \\Pi_D^k$であることに注意しましょう）\n",
    "\n",
    "Good eventのとき、\n",
    "\n",
    "$$\n",
    "V_1^{\\pi_k}\\left(s_1 ; {c}, \\widetilde{p}_k\\right) \\leq V_1^{\\star}\\left(s_1\\right)\n",
    "$$\n",
    "\n",
    "および\n",
    "\n",
    "$$\n",
    "V_1^{\\pi_k}\\left(s_1 ; {c}, \\widetilde{p}_k\\right) \\leq V_1^{\\pi_k}\\left(s_1 ; c, p\\right)\n",
    "$$\n",
    "\n",
    "であることを示しましょう。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V^{\\pi_k}\\left(s_1; c, p\\right)\n",
    "& \\geq \\min _{\\pi \\in \\Delta_A^S}\\left\\{c^T q^\\pi(p) \\mid \\pi \\in \\Pi_D\\right\\} \n",
    "= V^*\\left(s_1\\right)\\\\\n",
    "& \\geq \\min _{\\pi \\in \\Delta_A^S, p^{\\prime} \\in B_k^p}\\left\\{c^T q^\\pi(p') \\mid \\pi \\in \\Pi_D^k\\right\\} \\\\\n",
    "& =\\min _{\\pi \\in \\Delta_A^S, p^{\\prime} \\in B_k^p}\\left\\{c^T q^\\pi(p') \\mid {D} q^\\pi\\left(p^{\\prime}\\right) \\leq \\alpha\\right\\} \n",
    "=V_1^{\\pi_k}\\left(s_1 ; {c}, \\widetilde{p}_k\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "なので、両方成立します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 拡張線形計画問題\n",
    "\n",
    "OptCMDPは次の問題を解くことを思い出しましょう。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{\\widetilde{p} \\in B_k^p, \\pi \\in \\Pi^{\\mathrm{MR}}} & \\sum_{h, s, a} {c}_h(s, a) q_h^\\pi(s, a ; \\widetilde{p}) \\\\\n",
    "\\text { s.t. } & \\sum_{h, s, a} {d}_{i, h}(s, a) q_h^\\pi(s, a ; \\widetilde{p}) \\leq \\alpha_i, \\quad \\forall i \\in[H]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "一方で、これはダイレクトに解くことはできません（目的変数が２つあるせい？）。\n",
    "\n",
    "そこで、\n",
    "\n",
    "$$z_h^\\pi\\left(s, a, s^{\\prime} ; p\\right)=p_h\\left(s^{\\prime} \\mid s, a\\right) q_h^\\pi(s, a ; p)$$\n",
    "\n",
    "で定義される変数に対する拡張線形計画問題を解きます。\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "\\min _z \\sum_{h, s, a, s^{\\prime}} z_h\\left(s, a, s^{\\prime}\\right) c_h(s, a) & \\\\\n",
    "\\text { s.t. } \\sum_{h, s, a, s^{\\prime}} z_h\\left(s, a, s^{\\prime}\\right) d_{i, h}(s, a) \\leq \\alpha_i & \\forall i \\in[I]\\\\\n",
    "\\sum_{a, s^{\\prime}} z_h\\left(s, a, s^{\\prime}\\right)=\\sum_{s^{\\prime}, a^{\\prime}} z_{h-1}\\left(s^{\\prime}, a^{\\prime}, s\\right) & \\forall h \\in[H] \\backslash\\{1\\} \\\\\n",
    "\\sum_{a, s^{\\prime}} z_1\\left(s, a, s^{\\prime}\\right)=\\mu(s) & \\forall s \\in \\mathcal{S}\\\\\n",
    "z_h\\left(s, a, s^{\\prime}\\right) \\geq 0 & \\forall\\left(s, a, s^{\\prime}, h\\right) \\in \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\times[H] \\\\\n",
    "z_h\\left(s, a, s^{\\prime}\\right)-\\left(\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)+\\beta_{h, k}^p\\left(s, a, s^{\\prime}\\right)\\right) \\sum_y z_h(s, a, y) \\leq 0 & \\forall\\left(s, a, s^{\\prime}, h\\right) \\in \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\times[H] \\\\\n",
    "-z_h\\left(s, a, s^{\\prime}\\right)+\\left(\\bar{p}_h^{k-1}\\left(s^{\\prime} \\mid s, a\\right)-\\beta_{h, k}^p\\left(s, a, s^{\\prime}\\right)\\right) \\sum_y z_h(s, a, y) \\leq 0 & \\forall\\left(s, a, s^{\\prime}, h\\right) \\in \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\times[H]\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "この解の$z$を次のようにすれば最適解が得られます。\n",
    "$$\\widetilde{p}_h^k\\left(s^{\\prime} \\mid s, a\\right)=\\frac{z\\left(s, a, s^{\\prime}\\right)}{\\sum_y z(s, a, y)} \\quad \\text{and} \\quad \\pi_k(a \\mid s)=\\frac{\\sum_{s^{\\prime}} z\\left(s, a, s^{\\prime}\\right)}{\\sum_{b, s^{\\prime}} z\\left(s, b, s^{\\prime}\\right)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このOptCMDPのリグレットを導出しましょう。\n",
    "\n",
    "$\\delta\\in (0, 1)$とします。このとき、$K'\\in [K]$について、確率$1-\\delta$以上で、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\operatorname{Reg}_{+}\\left(K^{\\prime} ; c\\right) \\leq \\widetilde{\\mathcal{O}}\\left(\\sqrt{S \\mathcal{N} H^4 K}+(\\sqrt{\\mathcal{N}}+H) H^2 S A\\right), \\\\\n",
    "& \\operatorname{Reg}_{+}\\left(K^{\\prime} ; d\\right) \\leq \\widetilde{\\mathcal{O}}\\left(\\sqrt{S \\mathcal{N} H^4 K}+(\\sqrt{\\mathcal{N}}+H) H^2 S A\\right) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立ちます。証明しましょう。\n",
    "\n",
    "まず、確率$1-\\delta$以上でGood Eventが成立します（Good event参照）。\n",
    "このとき、\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text { Regret }^{+}\\left(K^{\\prime} ; c\\right) & =\\sum_k\\left[V_1^{\\pi_k}\\left(s_1 ; c, p\\right)-V_1^*\\left(s_1 ; c, p\\right)\\right]_{+} \\leq \\sum_k\\left[V_1^{\\pi_k}\\left(s_1 ; c, p\\right)-V_1^{\\pi_k}\\left(s_1 ; \\widetilde{c}_k, \\widetilde{p}_k\\right)\\right]_{+} \\\\\n",
    "& =\\sum_k V_1^{\\pi_k}\\left(s_1 ; c, p\\right)-V_1^{\\pi_k}\\left(s_1 ; \\widetilde{c}_k, \\widetilde{p}_k\\right) \\\\\n",
    "& \\leq \\widetilde{\\mathcal{O}}\\left(\\sqrt{S \\mathcal{N} H^4 K}+(\\sqrt{\\mathcal{N}}+H) H^2 S A\\right) .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立ちます。\n",
    "ここで、最初の不等式と２つ目の等式は楽観的方策のおかげです。最後の不等式はAppendix Eを参照してください。（TODO: Appendix Eは何かと便利そうなのでまとめておこう。）\n",
    "\n",
    "続いて制約違反のリグレットを導出します。\n",
    "これも似たような変形でできます。\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sum_{k=1}^{K^{\\prime}}\\left[V_1^{\\pi_k}\\left(s_1 ; d_i, p\\right)-\\alpha_i\\right]_{+} & =\\sum_{k=1}^{K^{\\prime}}[\\underbrace{V_1^{\\pi_k}\\left(s_1 ; d_i, p\\right)-V_1^{\\pi_k}\\left(s_1 ; \\widetilde{d}_i^k, \\tilde{p}^k\\right)}_{\\geq 0}+\\underbrace{V_1^{\\pi_k}\\left(s_1 ; \\tilde{d}_i^k, \\tilde{p}^k\\right)-\\alpha_i}_{\\leq 0}]_{+} \\\\\n",
    "& \\leq \\sum_{k=1}^{K^{\\prime}} V_1^{\\pi_k}\\left(s_1 ; d_i\\right)-V_1^{\\pi_k}\\left(s_1 ; \\tilde{d}_i^k, \\widetilde{p}^k\\right) \\\\\n",
    "& \\leq \\widetilde{\\mathcal{O}}\\left(\\sqrt{S \\mathcal{N} H^4 K}+(\\sqrt{\\mathcal{N}}+H) H^2 S A\\right) .\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PythonでOptCMDPを実装してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "ホライゾン： 10\n",
      "制約： 3.0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "from jax.random import PRNGKey\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "H = 10  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "rew = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# コスト行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "cost = jax.random.uniform(key=key, shape=(H, S, A))\n",
    "assert cost.shape == (H, S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "P = jax.random.uniform(key=key, shape=(H, S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "key, _ = jax.random.split(key)\n",
    "init_dist = jax.random.uniform(key, shape=(S,))\n",
    "init_dist = init_dist / jnp.sum(init_dist)\n",
    "np.testing.assert_allclose(init_dist.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class CMDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    cost: jnp.array  # 報酬行列\n",
    "    const: float  # 制約の閾値\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    init_dist: jnp.array  # 初期分布\n",
    "    optimal_Q_rew: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "const = 0.3 * H  # 制約は適当です。このときに実行可能である保証はとくにありません。\n",
    "mdp = CMDP(S_set, A_set, H, rew, cost, const, P, init_dist)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)\n",
    "print(\"制約：\", mdp.const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: CMDP, policy: jnp.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (CMDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_Q_rew (jnp.ndarray): (HxSxA)の行列. 報酬関数についてのQ\n",
    "        policy_Q_cost (jnp.ndarray): (HxSxA)の行列. コスト関数についてのQ\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, args):\n",
    "        policy_Q, g = args\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h+1] * policy_Q[h+1]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h].set(g[h] + next_v)\n",
    "        return policy_Q, g\n",
    "    \n",
    "    policy_Q_rew = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_rew, mdp.rew\n",
    "    policy_Q_rew, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "\n",
    "    policy_Q_cost = jnp.zeros((H+1, S, A))\n",
    "    args = policy_Q_cost, mdp.cost\n",
    "    policy_Q_cost, _ = jax.lax.fori_loop(0, mdp.H, backup, args)\n",
    "    return policy_Q_rew[:-1], policy_Q_cost[:-1]\n",
    "\n",
    "\n",
    "uni_policy = jnp.ones((H, S, A)) / A\n",
    "_ = compute_policy_Q(mdp, uni_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは探索無しで、CMDPの最適方策を求めてみます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the CBC MILP Solver \n",
      "Version: 2.10.3 \n",
      "Build Date: Dec 15 2019 \n",
      "\n",
      "command line - /home/toshinori/.cache/pypoetry/virtualenvs/shumi-VTLwuKSy-py3.9/lib/python3.9/site-packages/pulp/solverdir/cbc/linux/64/cbc /tmp/c6618811375d4308b2934efba067f2af-pulp.mps max timeMode elapsed branch printingOptions all solution /tmp/c6618811375d4308b2934efba067f2af-pulp.sol (default strategy 1)\n",
      "At line 2 NAME          MODEL\n",
      "At line 3 ROWS\n",
      "At line 106 COLUMNS\n",
      "At line 3707 RHS\n",
      "At line 3809 BOUNDS\n",
      "At line 3810 ENDATA\n",
      "Problem MODEL has 101 rows, 300 columns and 3300 elements\n",
      "Coin0008I MODEL read with 0 errors\n",
      "Option for timeMode changed from cpu to elapsed\n",
      "Presolve 101 (0) rows, 300 (0) columns and 3300 (0) elements\n",
      "0  Obj -0 Primal inf 1.4382028 (10) Dual inf 380.18679 (300)\n",
      "0  Obj -0 Primal inf 1.4382028 (10) Dual inf 1.9122887e+12 (300)\n",
      "31  Obj -0 Primal inf 1.4382028 (10) Dual inf 2.3956333e+12 (208)\n",
      "62  Obj -0 Primal inf 1.4382028 (10) Dual inf 2.9382449e+12 (127)\n",
      "93  Obj 1.1159012 Primal inf 1.1032446 (7) Dual inf 1.1845622e+12 (69)\n",
      "136  Obj 5.6549449 Primal inf 0.012776934 (1) Dual inf 3.216376e+10 (40)\n",
      "167  Obj 6.5938636 Dual inf 1.0021614 (7)\n",
      "176  Obj 6.6539402\n",
      "Optimal - objective value 6.6539402\n",
      "Optimal objective 6.653940213 - 176 iterations time 0.002\n",
      "Option for printingOptions changed from normal to all\n",
      "Total time (CPU seconds):       0.00   (Wallclock seconds):       0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pulp\n",
    "from itertools import product\n",
    "prob = pulp.LpProblem(name=\"CMDP\", sense=pulp.LpMaximize)\n",
    "hsa_indices = [(h, s, a) for h, s, a in product(range(H), range(S), range(A))]\n",
    "sa_indices = [(s, a) for s, a in product(range(S), range(A))]\n",
    "d = pulp.LpVariable.dicts(\"d\", hsa_indices, lowBound=0, cat=\"Continuous\")\n",
    "\n",
    "# 目的関数\n",
    "prob += pulp.lpSum([d[hsa] * mdp.rew[hsa[0], hsa[1], hsa[2]] for hsa in hsa_indices])\n",
    "\n",
    "# 初期状態についての制約\n",
    "for s in range(S):\n",
    "    d_0sa = [d[(0, s, a)] for a in range(A)]\n",
    "    prob += pulp.lpSum(d_0sa) == mdp.init_dist[s].item()\n",
    "\n",
    "# 各ステップについての制約\n",
    "for h in range(1, H):\n",
    "    for ns in range(S):\n",
    "        d_hns = pulp.lpSum([d[(h, ns, na)] for na in range(A)])\n",
    "        d_phns = pulp.lpSum([d[(h-1, sa[0], sa[1])] * mdp.P[h-1, sa[0], sa[1], ns] for sa in sa_indices])\n",
    "        prob += d_hns == d_phns\n",
    "\n",
    "# CMDPとしての制約\n",
    "const = pulp.lpSum([d[hsa] * mdp.cost[hsa[0], hsa[1], hsa[2]] for hsa in hsa_indices]) <= mdp.const\n",
    "prob += const\n",
    "\n",
    "\n",
    "sol = prob.solve()\n",
    "d_arr = jnp.array([pulp.value(d[h, s, a]) for (h, s, a) in hsa_indices])\n",
    "d_arr = d_arr.reshape(H, S, A)\n",
    "\n",
    "np.testing.assert_allclose(d_arr.sum(axis=(1, 2)), 1.0, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適方策の累積コスト和 3.0\n",
      "最適方策の累積報酬和 6.65394\n"
     ]
    }
   ],
   "source": [
    "optimal_policy = d_arr / d_arr.sum(axis=-1, keepdims=True)\n",
    "Q_rew, Q_cost = compute_policy_Q(mdp, optimal_policy)\n",
    "\n",
    "total_cost = (Q_cost * optimal_policy)[0].sum(axis=-1) @ mdp.init_dist\n",
    "assert total_cost <= mdp.const\n",
    "print(\"最適方策の累積コスト和\", total_cost)\n",
    "\n",
    "total_rew = (Q_rew * optimal_policy)[0].sum(axis=-1) @ mdp.init_dist\n",
    "print(\"最適方策の累積報酬和\", total_rew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一様方策の累積コスト和 4.9952\n",
      "一様方策の累積報酬和 4.922522\n"
     ]
    }
   ],
   "source": [
    "uniform_policy = jnp.ones((H, S, A)) / A\n",
    "Q_rew, Q_cost = compute_policy_Q(mdp, uniform_policy)\n",
    "\n",
    "total_cost = (Q_cost * uniform_policy)[0].sum(axis=-1) @ mdp.init_dist\n",
    "print(\"一様方策の累積コスト和\", total_cost)\n",
    "\n",
    "total_rew = (Q_rew * uniform_policy)[0].sum(axis=-1) @ mdp.init_dist\n",
    "print(\"一様方策の累積報酬和\", total_rew)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最適方策は計算できました。OptCMDPを実装しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shumi-VTLwuKSy-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
