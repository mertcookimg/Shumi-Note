{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習の実験に便利なコード\n",
    "\n",
    "タイトルの通りです。よく使う関数をまとめます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 無限ホライゾン"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルコフ決定過程の生成\n",
    "\n",
    "[強化学習の青本](https://amzn.asia/d/2epmlxT)に従ったMDPの定義用のコードです。\n",
    "MDPを次で定義します。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. 遷移確率行列: $P\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. 報酬行列: $r\\in \\mathbb{R}^{S\\times A}$\n",
    "5. 割引率: $\\gamma \\in [0, 1)$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "割引率： 0.8\n",
      "ホライゾン： 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = np.arange(S)  # 状態集合\n",
    "A_set = np.arange(A)  # 行動集合\n",
    "gamma = 0.8  # 割引率\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "rew = np.random.rand(S, A)\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "P = np.random.rand(S*A, S)\n",
    "P = P / np.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_almost_equal(P.sum(axis=-1), 1)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "mu = np.random.rand(S)\n",
    "mu = mu / np.sum(mu)\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: np.array  # 状態集合\n",
    "    A_set: np.array  # 行動集合\n",
    "    gamma: float  # 割引率\n",
    "    H: int  # エフェクティブホライゾン\n",
    "    rew: np.array  # 報酬行列\n",
    "    P: np.array  # 遷移確率行列\n",
    "    mu: np.array  # 初期分布\n",
    "    optimal_Q: Optional[np.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "H = int(1 / (1 - gamma))\n",
    "mdp = MDP(S_set, A_set, gamma, H, rew, P, mu)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"割引率：\", mdp.gamma)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動的計画法\n",
    "\n",
    "参考\n",
    "\n",
    "* [Safe Policy Iteration](http://proceedings.mlr.press/v28/pirotta13.pdf)の２ページ目\n",
    "\n",
    "**表記**\n",
    "\n",
    "* 内積の記法: $f_1, f_2 \\in \\mathbb{R}^{S\\times A}$に対して、$\\langle f_1, f_2 \\rangle = (\\sum_{a\\in A} f_1(s, a)f_2(s, a))_s \\in \\mathbb{R}^S$とします。これは方策についての和を省略するときなどに便利です。例えば$\\langle \\pi, q_\\pi\\rangle = v_\\pi$です。\n",
    "* 方策行列（$\\Pi^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi^\\pi(s,(s, a))=\\pi(a \\mid s)$ \n",
    "    * $\\Pi^\\pi q^\\pi = \\langle \\pi, q^\\pi \\rangle = v^\\pi$が成立。\n",
    "* 遷移確率行列１（$P^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P^\\pi = P \\Pi^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q^\\pi = r + \\gamma P^\\pi q^\\pi$が成立。\n",
    "    * $(I - \\gamma P^\\pi)^{-1}r = q^\\pi$が成立する。\n",
    "* 遷移確率行列２（$\\bar{P}^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}^\\pi = \\Pi^\\pi P$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v^\\pi = \\Pi^\\pi r + \\gamma \\bar{P}^\\pi v^\\pi$。\n",
    "    * $(I - \\gamma \\bar{P}^\\pi)^{-1}\\Pi^\\pi r = v^\\pi$が成立する。\n",
    "* 割引訪問頻度１（$d^\\pi_\\mu \\in \\mathbb{R}^{SA}$）：S, Aについての割引累積訪問頻度\n",
    "    * $d^\\pi_\\mu = \\mu \\Pi^\\pi (I - \\gamma P^\\pi)^{-1} = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1} \\Pi^\\pi$\n",
    "    * ${d}^\\pi_\\mu (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$が成立する。\n",
    "* 割引訪問頻度２（$\\bar{d}^\\pi_\\mu \\in \\mathbb{R}^{S}$）：Sについての割引累積訪問頻度\n",
    "    * $\\bar{d}^\\pi_\\mu = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1}$\n",
    "    * $\\bar{d}^\\pi_\\mu (s) = \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$が成立する。\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi}$を返します。\n",
    "* ``compute_policy_visit``: 方策 $\\pi$ の割引訪問頻度２$\\bar{d}^\\pi_\\mu \\in \\mathbb{R}^{S}$ を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qベースの動的計画法と逆行列の解の差： 4.7683716e-07\n",
      "Vベースの動的計画法と逆行列の解の差： 4.7683716e-07\n",
      "動的計画法で計算した割引訪問頻度と逆行列の解の差 1.1920929e-07\n",
      "割引訪問頻度で計算した期待リターンと動的計画法の解の差 2.3841858e-07\n",
      "SAについての割引訪問頻度の求め方２つの差： 5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: np.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (np.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (np.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    S, A = Q.shape\n",
    "    greedy_policy = greedy_policy.at[jnp.arange(S), Q.argmax(axis=1)].set(1)\n",
    "    assert greedy_policy.shape == (S, A)\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, S: int, A: int):\n",
    "    \"\"\"MDPについて、ベルマン最適作用素を複数回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (np.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        next_v = mdp.P @ optimal_Q.max(axis=1)\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, optimal_Q)\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: np.ndarray):\n",
    "    \"\"\"MDPと方策について、ベルマン期待作用素を複数回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (np.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (np.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "\n",
    "    def backup(policy_Q):\n",
    "        max_Q = (policy * policy_Q).sum(axis=1)\n",
    "        next_v = mdp.P @ max_Q\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    policy_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, policy_Q)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: np.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (np.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (np.ndarray): (SxSA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    PI = policy.reshape(1, S, A)\n",
    "    PI = jnp.tile(PI, (S, 1, 1))\n",
    "    eyes = jnp.eye(S).reshape(S, S, 1)\n",
    "    PI = (eyes * PI).reshape(S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit(mdp: MDP, policy: np.ndarray, init_dist: np.ndarray):\n",
    "    \"\"\"MDPと方策について、割引訪問頻度を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (np.ndarray): (SxA)の行列\n",
    "        init_dist (np.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (np.ndarray): (S)のベクトル\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    PiP = Pi @ mdp.P.reshape(S*A, S) \n",
    "\n",
    "    def backup(visit):\n",
    "        next_visit = mdp.gamma * visit @ PiP\n",
    "        return init_dist + next_visit\n",
    "    \n",
    "    body_fn = lambda i, visit: backup(visit)\n",
    "    visit = jax.lax.fori_loop(0, mdp.H + 100, body_fn, init_dist)\n",
    "    return visit\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "\n",
    "\n",
    "# 逆行列による解法 Q\n",
    "Pi = compute_policy_matrix(optimal_policy)\n",
    "PPi = mdp.P.reshape(S*A, S) @ Pi\n",
    "optimal_Q_inv = np.linalg.inv(np.eye(S*A) - mdp.gamma * PPi) @ mdp.rew.reshape(S*A)\n",
    "print(\"Qベースの動的計画法と逆行列の解の差：\", np.abs(optimal_Q_inv - optimal_Q_DP.reshape(-1)).max())\n",
    "\n",
    "\n",
    "# 逆行列による解法 V\n",
    "Pi = compute_policy_matrix(optimal_policy)\n",
    "PiP = Pi @ mdp.P.reshape(S*A, S) \n",
    "Pirew = Pi @ mdp.rew.reshape(S*A)\n",
    "optimal_V_inv = np.linalg.inv(np.eye(S) - mdp.gamma * PiP) @ Pirew\n",
    "print(\"Vベースの動的計画法と逆行列の解の差：\", np.abs(optimal_V_inv - optimal_V_DP.reshape(-1)).max())\n",
    "\n",
    "\n",
    "# 割引訪問頻度の計算\n",
    "d_pi_DP = compute_policy_visit(mdp, optimal_policy, mdp.mu)\n",
    "d_pi_inv = mdp.mu @ np.linalg.inv(np.eye(S) - mdp.gamma * PiP)\n",
    "print(\"動的計画法で計算した割引訪問頻度と逆行列の解の差\", np.abs(d_pi_DP - d_pi_inv).max())\n",
    "optimal_return_DP = mdp.mu @ optimal_V_DP\n",
    "optimal_return_visit = d_pi_inv @ Pirew\n",
    "print(\"割引訪問頻度で計算した期待リターンと動的計画法の解の差\", np.abs(optimal_return_DP - optimal_return_visit).max())\n",
    "\n",
    "\n",
    "d_pi_inv_SA1 = mdp.mu @ np.linalg.inv(np.eye(S) - mdp.gamma * PiP) @ Pi\n",
    "d_pi_inv_SA2 = mdp.mu @ Pi @ np.linalg.inv(np.eye(S*A) - mdp.gamma * PPi)\n",
    "\n",
    "print(\"SAについての割引訪問頻度の求め方２つの差：\", np.abs(d_pi_inv_SA1 - d_pi_inv_SA2).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有限ホライゾン"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルコフ決定過程の生成\n",
    "\n",
    "**参考**\n",
    "\n",
    "有限MDPの定義については[Reinforcement Learning: Theory and Algorithms](https://rltheorybook.github.io/)の1.2章を参照しています。\n",
    "有限ホライゾンの場合、遷移行列や報酬関数が各ステップで変わる設定を考えます。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. $h$ステップ目の遷移確率行列: $P_h\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. $h$ステップ目の報酬行列: $r_h\\in \\mathbb{R}^{S\\times A}$\n",
    "5. ホライゾン: $H$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "ホライゾン： 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = np.arange(S)  # 状態集合\n",
    "A_set = np.arange(A)  # 行動集合\n",
    "H = 5  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "rew = np.random.rand(H, S, A)\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "P = np.random.rand(H, S*A, S)\n",
    "P = P / np.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_almost_equal(P.sum(axis=-1), 1)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "mu = np.random.rand(S)\n",
    "mu = mu / np.sum(mu)\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: np.array  # 状態集合\n",
    "    A_set: np.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: np.array  # 報酬行列\n",
    "    P: np.array  # 遷移確率行列\n",
    "    mu: np.array  # 初期分布\n",
    "    optimal_Q: Optional[np.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, H, rew, P, mu)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動的計画法\n",
    "\n",
    "**表記**\n",
    "\n",
    "* ステップ$h$の方策行列（$\\Pi_h^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi_h, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi_h^\\pi(s,(s, a))=\\pi_h(a \\mid s)$ \n",
    "    * $\\Pi_h^\\pi q_h^\\pi = \\langle \\pi, q_h^\\pi \\rangle = v_h^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列１（$P_h^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P_h^\\pi = P_h \\Pi_h^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q_h^\\pi = r_h + P_h^\\pi q^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列２（$\\bar{P}_h^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}_h^\\pi = \\Pi_h^\\pi P_h$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v_h^\\pi = \\Pi^\\pi r_h + \\gamma \\bar{P}_h^\\pi v^\\pi$。\n",
    "\n",
    "TODO: 有限ホライゾンのときは訪問分布の扱いがよくわかんないな。時間非定常のときはリターンの計算には使えない？\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($H\\times S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi} : H \\times S \\times SA$を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数と最適方策の価値関数の差 0.0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: np.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (np.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    H, S, A = Q.shape\n",
    "    \n",
    "    def body_fn(i, greedy_policy):\n",
    "        greedy_policy = greedy_policy.at[i, jnp.arange(S), Q[i].argmax(axis=-1)].set(1)\n",
    "        return greedy_policy\n",
    "\n",
    "    greedy_policy = jax.lax.fori_loop(0, H, body_fn, greedy_policy)\n",
    "    assert greedy_policy.shape == (H, S, A)\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, H: int, S: int, A: int):\n",
    "    \"\"\"ベルマン最適作用素をホライゾン回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (np.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(i, optimal_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = optimal_Q[h].max(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        assert next_v.shape == (S, A)\n",
    "        optimal_Q = optimal_Q.at[h-1].set(mdp.rew[h] + next_v)\n",
    "        return optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((H, S, A))\n",
    "    return jax.lax.fori_loop(0, mdp.H, backup, optimal_Q)\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.H, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: np.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (np.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, policy_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h] * policy_Q[h]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        assert next_v.shape == (S, A)\n",
    "        policy_Q = policy_Q.at[h-1].set(mdp.rew[h] + next_v)\n",
    "        return policy_Q\n",
    "    \n",
    "    policy_Q = jnp.zeros((H, S, A))\n",
    "    return jax.lax.fori_loop(0, mdp.H, backup, policy_Q)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: np.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (np.ndarray): (HxSxSA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    PI = policy.reshape(H, 1, S, A)\n",
    "    PI = jnp.tile(PI, (1, S, 1, 1))\n",
    "    eyes = jnp.tile(jnp.eye(S).reshape(1, S, S, 1), (H, 1, 1, 1))\n",
    "    PI = (eyes * PI).reshape(H, S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=-1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "optimal_policy_Q_DP = compute_policy_Q(mdp, optimal_policy)\n",
    "print(\"最適価値関数と最適方策の価値関数の差\", np.abs(optimal_Q_DP - optimal_policy_Q_DP).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('shumi-VTLwuKSy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6b7cac5e0d2ff733f340da4d53ae5ecfef7f7ad39623f5982b029a09306b36b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
