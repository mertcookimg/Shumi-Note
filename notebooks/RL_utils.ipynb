{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 強化学習の実験に便利なコード\n",
    "\n",
    "タイトルの通りです。よく使う関数をまとめます。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 無限ホライゾン"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルコフ決定過程の生成\n",
    "\n",
    "[強化学習の青本](https://amzn.asia/d/2epmlxT)に従ったMDPの定義用のコードです。\n",
    "MDPを次で定義します。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. 遷移確率行列: $P\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. 報酬行列: $r\\in \\mathbb{R}^{S\\times A}$\n",
    "5. 割引率: $\\gamma \\in [0, 1)$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "割引率： 0.8\n",
      "ホライゾン： 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.random import PRNGKey\n",
    "import jax\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "key = PRNGKey(0)\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "gamma = 0.8  # 割引率\n",
    "\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "rew = jax.random.uniform(key=key, shape=(S, A))\n",
    "assert rew.shape == (S, A)\n",
    "\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "P = jax.random.uniform(key=key, shape=(S*A, S))\n",
    "P = P / jnp.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "mu = np.random.rand(S)\n",
    "mu = mu / np.sum(mu)\n",
    "np.testing.assert_allclose(mu.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    gamma: float  # 割引率\n",
    "    H: int  # エフェクティブホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    mu: jnp.array  # 初期分布\n",
    "    optimal_Q: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "H = int(1 / (1 - gamma))\n",
    "mdp = MDP(S_set, A_set, gamma, H, rew, P, mu)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"割引率：\", mdp.gamma)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動的計画法\n",
    "\n",
    "参考\n",
    "\n",
    "* [Safe Policy Iteration](http://proceedings.mlr.press/v28/pirotta13.pdf)の２ページ目\n",
    "\n",
    "**表記**\n",
    "\n",
    "* 内積の記法: $f_1, f_2 \\in \\mathbb{R}^{S\\times A}$に対して、$\\langle f_1, f_2 \\rangle = (\\sum_{a\\in A} f_1(s, a)f_2(s, a))_s \\in \\mathbb{R}^S$とします。これは方策についての和を省略するときなどに便利です。例えば$\\langle \\pi, q_\\pi\\rangle = v_\\pi$です。\n",
    "* 方策行列（$\\Pi^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi^\\pi(s,(s, a))=\\pi(a \\mid s)$ \n",
    "    * $\\Pi^\\pi q^\\pi = \\langle \\pi, q^\\pi \\rangle = v^\\pi$が成立。\n",
    "* 遷移確率行列１（$P^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P^\\pi = P \\Pi^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q^\\pi = r + \\gamma P^\\pi q^\\pi$が成立。\n",
    "    * $(I - \\gamma P^\\pi)^{-1}r = q^\\pi$が成立する。\n",
    "* 遷移確率行列２（$\\bar{P}^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}^\\pi = \\Pi^\\pi P$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v^\\pi = \\Pi^\\pi r + \\gamma \\bar{P}^\\pi v^\\pi$。\n",
    "    * $(I - \\gamma \\bar{P}^\\pi)^{-1}\\Pi^\\pi r = v^\\pi$が成立する。\n",
    "* 割引訪問頻度１（$d^\\pi_\\mu \\in \\mathbb{R}^{SA}$）：S, Aについての割引累積訪問頻度\n",
    "    * $d^\\pi_\\mu = \\mu \\Pi^\\pi (I - \\gamma P^\\pi)^{-1} = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1} \\Pi^\\pi$\n",
    "    * ${d}^\\pi_\\mu (s, a) = \\pi(a|s) \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$が成立する。\n",
    "* 割引訪問頻度２（$\\bar{d}^\\pi_\\mu \\in \\mathbb{R}^{S}$）：Sについての割引累積訪問頻度\n",
    "    * $\\bar{d}^\\pi_\\mu = \\mu (I - \\gamma \\bar{P}^\\pi)^{-1}$\n",
    "    * $\\bar{d}^\\pi_\\mu (s) = \\sum_{s_0} \\mu(s_0) \\sum_{t=0}^\\infty \\mathrm{Pr}\\left(S_t=s|S_0=s_0, M(\\pi)\\right)$が成立する。\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi}$を返します。\n",
    "* ``compute_policy_visit``: 方策 $\\pi$ の割引訪問頻度２$\\bar{d}^\\pi_\\mu \\in \\mathbb{R}^{S}$ を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qベースの動的計画法と逆行列の解の差： 4.7683716e-07\n",
      "Vベースの動的計画法と逆行列の解の差： 7.1525574e-07\n",
      "動的計画法で計算した割引訪問頻度と逆行列の解の差 1.7881393e-07\n",
      "割引訪問頻度で計算した期待リターンと動的計画法の解の差 2.3841858e-07\n",
      "SAについての割引訪問頻度の求め方２つの差： 5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: jnp.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    S, A = Q.shape\n",
    "    greedy_policy = greedy_policy.at[jnp.arange(S), Q.argmax(axis=1)].set(1)\n",
    "    assert greedy_policy.shape == (S, A)\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, S: int, A: int):\n",
    "    \"\"\"MDPについて、ベルマン最適作用素を複数回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(optimal_Q):\n",
    "        next_v = mdp.P @ optimal_Q.max(axis=1)\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    optimal_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, optimal_Q)\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、ベルマン期待作用素を複数回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (jnp.ndarray): (SxA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    chex.assert_shape(policy, (mdp.S, mdp.A))\n",
    "\n",
    "    def backup(policy_Q):\n",
    "        max_Q = (policy * policy_Q).sum(axis=1)\n",
    "        next_v = mdp.P @ max_Q\n",
    "        assert next_v.shape == (S, A)\n",
    "        return mdp.rew + mdp.gamma * next_v\n",
    "    \n",
    "    policy_Q = jnp.zeros((S, A))\n",
    "    body_fn = lambda i, Q: backup(Q)\n",
    "    return jax.lax.fori_loop(0, mdp.H + 100, body_fn, policy_Q)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: jnp.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (jnp.ndarray): (SxSA)の行列\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    PI = policy.reshape(1, S, A)\n",
    "    PI = jnp.tile(PI, (S, 1, 1))\n",
    "    eyes = jnp.eye(S).reshape(S, S, 1)\n",
    "    PI = (eyes * PI).reshape(S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_visit(mdp: MDP, policy: jnp.ndarray, init_dist: jnp.ndarray):\n",
    "    \"\"\"MDPと方策について、割引訪問頻度を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (jnp.ndarray): (SxA)の行列\n",
    "        init_dist (jnp.ndarray): (S) 初期状態の分布\n",
    "\n",
    "    Returns:\n",
    "        visit (jnp.ndarray): (S)のベクトル\n",
    "    \"\"\"\n",
    "    S, A = policy.shape\n",
    "    chex.assert_shape(policy, (mdp.S, mdp.A))\n",
    "    Pi = compute_policy_matrix(policy)\n",
    "    PiP = Pi @ mdp.P.reshape(S*A, S) \n",
    "\n",
    "    def backup(visit):\n",
    "        next_visit = mdp.gamma * visit @ PiP\n",
    "        return init_dist + next_visit\n",
    "    \n",
    "    body_fn = lambda i, visit: backup(visit)\n",
    "    visit = jax.lax.fori_loop(0, mdp.H + 100, body_fn, init_dist)\n",
    "    return visit\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "mdp = mdp._replace(optimal_Q=optimal_Q_DP)\n",
    "\n",
    "\n",
    "# 逆行列による解法 Q\n",
    "Pi = compute_policy_matrix(optimal_policy)\n",
    "PPi = mdp.P.reshape(S*A, S) @ Pi\n",
    "optimal_Q_inv = jnp.linalg.inv(jnp.eye(S*A) - mdp.gamma * PPi) @ mdp.rew.reshape(S*A)\n",
    "print(\"Qベースの動的計画法と逆行列の解の差：\", jnp.abs(optimal_Q_inv - optimal_Q_DP.reshape(-1)).max())\n",
    "\n",
    "\n",
    "# 逆行列による解法 V\n",
    "Pi = compute_policy_matrix(optimal_policy)\n",
    "PiP = Pi @ mdp.P.reshape(S*A, S) \n",
    "Pirew = Pi @ mdp.rew.reshape(S*A)\n",
    "optimal_V_inv = jnp.linalg.inv(jnp.eye(S) - mdp.gamma * PiP) @ Pirew\n",
    "print(\"Vベースの動的計画法と逆行列の解の差：\", jnp.abs(optimal_V_inv - optimal_V_DP.reshape(-1)).max())\n",
    "\n",
    "\n",
    "# 割引訪問頻度の計算\n",
    "d_pi_DP = compute_policy_visit(mdp, optimal_policy, mdp.mu)\n",
    "d_pi_inv = mdp.mu @ jnp.linalg.inv(jnp.eye(S) - mdp.gamma * PiP)\n",
    "print(\"動的計画法で計算した割引訪問頻度と逆行列の解の差\", jnp.abs(d_pi_DP - d_pi_inv).max())\n",
    "optimal_return_DP = mdp.mu @ optimal_V_DP\n",
    "optimal_return_visit = d_pi_inv @ Pirew\n",
    "print(\"割引訪問頻度で計算した期待リターンと動的計画法の解の差\", jnp.abs(optimal_return_DP - optimal_return_visit).max())\n",
    "\n",
    "\n",
    "d_pi_inv_SA1 = mdp.mu @ jnp.linalg.inv(jnp.eye(S) - mdp.gamma * PiP) @ Pi\n",
    "d_pi_inv_SA2 = mdp.mu @ Pi @ jnp.linalg.inv(jnp.eye(S*A) - mdp.gamma * PPi)\n",
    "\n",
    "print(\"SAについての割引訪問頻度の求め方２つの差：\", jnp.abs(d_pi_inv_SA1 - d_pi_inv_SA2).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習用\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``sample_next_state``: 状態・行動の集合$D$のそれぞれについて次状態を$N$個返します\n",
    "* ``collect_samples_greedy``: $q\\in \\mathbb{R}^{S\\times A}$の貪欲方策で$N$回インタラクションしてサンプルを集めます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.random import PRNGKey\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"N\",))\n",
    "def sample_next_state(mdp: MDP, N: int, key: PRNGKey, D: jnp.array):\n",
    "    \"\"\" 遷移行列Pに従って次の状態をN個サンプルします\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        N (int): サンプルする個数\n",
    "        key (PRNGKey)\n",
    "        D (np.ndarray): 状態行動対の集合 [(s1, a1), (s2, a2), ...]\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        next_s_set (np.ndarray): (len(D) x N) の次状態の集合\n",
    "    \"\"\"\n",
    "    new_key, key = jax.random.split(key)\n",
    "    keys = jax.random.split(key, num=len(D))\n",
    "\n",
    "    @jax.vmap\n",
    "    def choice(key, sa):\n",
    "        return jax.random.choice(key, mdp.S_set, shape=(N,), p=P[sa[0], sa[1]])\n",
    "\n",
    "    next_s = choice(keys, D)\n",
    "    return new_key, next_s\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "N = 20000\n",
    "D = jnp.array([(1, 2), (2, 1), (0, 0), (3, 1)])\n",
    "key, next_states = sample_next_state(mdp, N, key, D)\n",
    "assert next_states.shape == (len(D), N)\n",
    "s, a = D[0]\n",
    "P0_approx = jnp.bincount(next_states[0], minlength=S) / N\n",
    "np.testing.assert_allclose(P0_approx, mdp.P[s, a], atol=1e-2)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"N\",))\n",
    "def collect_samples(mdp: MDP, N: int, key: PRNGKey, q: jnp.array, init_s: int):\n",
    "    \"\"\" MDPとインタラクションしてサンプルをN個集めます。qの貪欲方策に従って動きます。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        N (int): サンプルする個数\n",
    "        key (PRNGKey)\n",
    "        q (jnp.ndarray): 行動価値関数\n",
    "        init_s (int): 初期状態\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        sars (jnp.ndarray): (状態, 行動, 報酬, 次状態) x N の軌跡\n",
    "    \"\"\"\n",
    "    chex.assert_shape(q, (mdp.S, mdp.A))\n",
    "\n",
    "    def body_fn(n, args):\n",
    "        key, sars, s = args\n",
    "        a = q[s].argmax()\n",
    "        r = mdp.rew[s, a]\n",
    "        D = jnp.array([(s, a),])\n",
    "        key, next_s = sample_next_state(mdp, 1, key, D)\n",
    "        next_s = next_s[0][0]\n",
    "        sars = sars.at[n].set((s, a, r, next_s))\n",
    "        return key, sars, next_s\n",
    "\n",
    "    sars = jnp.zeros((N, 4))\n",
    "    args = key, sars, init_s\n",
    "    key, sars, next_s = jax.lax.fori_loop(0, N, body_fn, args)\n",
    "    return key, sars, next_s\n",
    "\n",
    "\n",
    "key, sars, next_s = collect_samples(mdp, N, key, mdp.optimal_Q, 0)\n",
    "assert sars.shape == (N, 4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 有限ホライゾン"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### マルコフ決定過程の生成\n",
    "\n",
    "**参考**\n",
    "\n",
    "有限MDPの定義については[Reinforcement Learning: Theory and Algorithms](https://rltheorybook.github.io/)の1.2章を参照しています。\n",
    "有限ホライゾンの場合、遷移行列や報酬関数が各ステップで変わる設定を考えます。\n",
    "\n",
    "1. 有限状態集合: $S=\\{1, \\dots, |S|\\}$\n",
    "2. 有限行動集合: $A=\\{1, \\dots, |A|\\}$\n",
    "3. $h$ステップ目の遷移確率行列: $P_h\\in \\mathbb{R}^{SA\\times S}$\n",
    "4. $h$ステップ目の報酬行列: $r_h\\in \\mathbb{R}^{S\\times A}$\n",
    "5. ホライゾン: $H$\n",
    "6. 初期状態: $\\mu \\in \\mathbb{R}^{S}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状態数： 10\n",
      "行動数： 3\n",
      "ホライゾン： 5\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from typing import NamedTuple, Optional\n",
    "\n",
    "\n",
    "S = 10  # 状態集合のサイズ\n",
    "A = 3  # 行動集合のサイズ\n",
    "S_set = jnp.arange(S)  # 状態集合\n",
    "A_set = jnp.arange(A)  # 行動集合\n",
    "H = 5  # ホライゾン\n",
    "\n",
    "# 報酬行列を適当に作ります\n",
    "rew = np.random.rand(H, S, A)\n",
    "assert rew.shape == (H, S, A)\n",
    "\n",
    "# 遷移確率行列を適当に作ります\n",
    "P = jax.random.uniform(key=key, shape=(H, S*A, S))\n",
    "P = P / np.sum(P, axis=-1, keepdims=True)  # 正規化して確率にします\n",
    "P = P.reshape(H, S, A, S)\n",
    "np.testing.assert_allclose(P.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "# 初期状態分布を適当に作ります\n",
    "mu = np.random.rand(S)\n",
    "mu = mu / np.sum(mu)\n",
    "np.testing.assert_allclose(mu.sum(axis=-1), 1, atol=1e-6)  # ちゃんと確率行列になっているか確認します\n",
    "\n",
    "\n",
    "# 状態集合, 行動集合, 割引率, 報酬行列, 遷移確率行列が準備できたのでMDPのクラスを作ります\n",
    "\n",
    "class MDP(NamedTuple):\n",
    "    S_set: jnp.array  # 状態集合\n",
    "    A_set: jnp.array  # 行動集合\n",
    "    H: int  # ホライゾン\n",
    "    rew: jnp.array  # 報酬行列\n",
    "    P: jnp.array  # 遷移確率行列\n",
    "    mu: jnp.array  # 初期分布\n",
    "    optimal_Q: Optional[jnp.ndarray] = None  # 最適Q値\n",
    "\n",
    "    @property\n",
    "    def S(self) -> int:  # 状態空間のサイズ\n",
    "        return len(self.S_set)\n",
    "\n",
    "    @property\n",
    "    def A(self) -> int:  # 行動空間のサイズ\n",
    "        return len(self.A_set)\n",
    "\n",
    "\n",
    "mdp = MDP(S_set, A_set, H, rew, P, mu)\n",
    "\n",
    "print(\"状態数：\", mdp.S)\n",
    "print(\"行動数：\", mdp.A)\n",
    "print(\"ホライゾン：\", mdp.H)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 動的計画法\n",
    "\n",
    "**表記**\n",
    "\n",
    "* ステップ$h$の方策行列（$\\Pi_h^\\pi \\in \\mathbb{R}^{S\\times SA}$）：$\\langle \\pi_h, q\\rangle$を行列で書きたいときに便利。\n",
    "    * $\\Pi_h^\\pi(s,(s, a))=\\pi_h(a \\mid s)$ \n",
    "    * $\\Pi_h^\\pi q_h^\\pi = \\langle \\pi, q_h^\\pi \\rangle = v_h^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列１（$P_h^\\pi \\in \\mathbb{R}^{SA\\times SA}$）: 次の状態についての方策の情報を追加したやつ。\n",
    "    * $P_h^\\pi = P_h \\Pi_h^\\pi$\n",
    "    * Q値を使ったベルマン期待作用素とかで便利。$q_h^\\pi = r_h + P_h^\\pi q^\\pi$が成立。\n",
    "* ステップ$h$の遷移確率行列２（$\\bar{P}_h^\\pi \\in \\mathbb{R}^{S\\times S}$）: 方策$\\pi$のもとでの状態遷移の行列。\n",
    "    * $\\bar{P}_h^\\pi = \\Pi_h^\\pi P_h$\n",
    "    * V値を使ったベルマン期待作用素とかで便利。$v_h^\\pi = \\Pi^\\pi r_h + \\gamma \\bar{P}_h^\\pi v^\\pi$。\n",
    "\n",
    "TODO: 有限ホライゾンのときは訪問分布の扱いがよくわかんないな。時間非定常のときはリターンの計算には使えない？\n",
    "\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "* ``compute_greedy_policy``: Q関数 ($H\\times S \\times A \\to \\mathcal{R}$) の貪欲方策を返します\n",
    "* ``compute_optimal_Q``: MDPの最適Q関数 $q_* : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_Q``: 方策 $\\pi$ のQ関数 $q_\\pi : H\\times S \\times A \\to \\mathcal{R}$ を返します。\n",
    "* ``compute_policy_matrix``: 方策$\\pi$の行列${\\Pi}^{\\pi} : H \\times S \\times SA$を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最適価値関数と最適方策の価値関数の差 0.0\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import chex\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_greedy_policy(Q: np.ndarray):\n",
    "    \"\"\"Q関数の貪欲方策を返します\n",
    "\n",
    "    Args:\n",
    "        Q (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        greedy_policy (np.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    greedy_policy = jnp.zeros_like(Q)\n",
    "    H, S, A = Q.shape\n",
    "    \n",
    "    def body_fn(i, greedy_policy):\n",
    "        greedy_policy = greedy_policy.at[i, jnp.arange(S), Q[i].argmax(axis=-1)].set(1)\n",
    "        return greedy_policy\n",
    "\n",
    "    greedy_policy = jax.lax.fori_loop(0, H, body_fn, greedy_policy)\n",
    "    chex.assert_shape(greedy_policy, (H, S, A))\n",
    "    return greedy_policy\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\", \"S\", \"A\"))\n",
    "def _compute_optimal_Q(mdp: MDP, H: int, S: int, A: int):\n",
    "    \"\"\"ベルマン最適作用素をホライゾン回走らせて最適価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (np.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "\n",
    "    def backup(i, optimal_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = optimal_Q[h].max(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        optimal_Q = optimal_Q.at[h-1].set(mdp.rew[h] + next_v)\n",
    "        return optimal_Q\n",
    "    \n",
    "    optimal_Q = jnp.zeros((H, S, A))\n",
    "    return jax.lax.fori_loop(0, mdp.H, backup, optimal_Q)\n",
    "\n",
    "compute_optimal_Q = lambda mdp: _compute_optimal_Q(mdp, mdp.H, mdp.S, mdp.A)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_Q(mdp: MDP, policy: np.ndarray):\n",
    "    \"\"\"ベルマン期待作用素をホライゾン回走らせて価値関数を動的計画法で計算します。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        optimal_Q (np.ndarray): (HxSxA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "\n",
    "    def backup(i, policy_Q):\n",
    "        h = H - i - 1\n",
    "        max_Q = (policy[h] * policy_Q[h]).sum(axis=1)\n",
    "        next_v = mdp.P[h] @ max_Q\n",
    "        chex.assert_shape(next_v, (S, A))\n",
    "        policy_Q = policy_Q.at[h-1].set(mdp.rew[h] + next_v)\n",
    "        return policy_Q\n",
    "    \n",
    "    policy_Q = jnp.zeros((H, S, A))\n",
    "    return jax.lax.fori_loop(0, mdp.H, backup, policy_Q)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def compute_policy_matrix(policy: np.ndarray):\n",
    "    \"\"\"\n",
    "    上で定義した方策行列を計算します。方策についての内積が取りたいときに便利です。\n",
    "    Args:\n",
    "        policy (np.ndarray): (HxSxA)の行列\n",
    "\n",
    "    Returns:\n",
    "        policy_matrix (np.ndarray): (HxSxSA)の行列\n",
    "    \"\"\"\n",
    "    H, S, A = policy.shape\n",
    "    PI = policy.reshape(H, 1, S, A)\n",
    "    PI = jnp.tile(PI, (1, S, 1, 1))\n",
    "    eyes = jnp.tile(jnp.eye(S).reshape(1, S, S, 1), (H, 1, 1, 1))\n",
    "    PI = (eyes * PI).reshape(H, S, S*A)\n",
    "    return PI\n",
    "\n",
    "\n",
    "# 動的計画法による最適価値関数\n",
    "optimal_Q_DP = compute_optimal_Q(mdp)\n",
    "optimal_V_DP = optimal_Q_DP.max(axis=-1)\n",
    "optimal_policy = compute_greedy_policy(optimal_Q_DP)\n",
    "optimal_policy_Q_DP = compute_policy_Q(mdp, optimal_policy)\n",
    "mdp = mdp._replace(optimal_Q=optimal_Q_DP)\n",
    "print(\"最適価値関数と最適方策の価値関数の差\", np.abs(optimal_Q_DP - optimal_policy_Q_DP).max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習用\n",
    "\n",
    "**実装した関数**\n",
    "\n",
    "有限ホライゾンは本質的にリセット機能がついているので、あんまりGenerative modelの仮定がありません。\n",
    "\n",
    "* ``sample_next_state``: ステップ・状態・行動の集合$D$のそれぞれについて次状態を$N$個返します\n",
    "* ``collect_samples_greedy``: $q\\in \\mathbb{R}^{H\\times S\\times A}$の貪欲方策で$H$回インタラクションしてサンプルを集めます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "p must be None or match the shape of a",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m N \u001b[39m=\u001b[39m \u001b[39m20000\u001b[39m\n\u001b[1;32m     30\u001b[0m D \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), (\u001b[39m4\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m)])\n\u001b[0;32m---> 31\u001b[0m key, next_states \u001b[39m=\u001b[39m sample_next_state(mdp, N, key, D)\n\u001b[1;32m     32\u001b[0m \u001b[39massert\u001b[39;00m next_states\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(D), N)\n\u001b[1;32m     33\u001b[0m h, s, a \u001b[39m=\u001b[39m D[\u001b[39m0\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[13], line 24\u001b[0m, in \u001b[0;36msample_next_state\u001b[0;34m(mdp, N, key, D)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mvmap\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchoice\u001b[39m(key, hsa):\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(key, mdp\u001b[39m.\u001b[39mS_set, shape\u001b[39m=\u001b[39m(N,), p\u001b[39m=\u001b[39mP[hsa[\u001b[39m0\u001b[39m], hsa[\u001b[39m1\u001b[39m], hsa[\u001b[39m2\u001b[39m]])\n\u001b[0;32m---> 24\u001b[0m next_s \u001b[39m=\u001b[39m choice(keys, D)\n\u001b[1;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m new_key, next_s\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[13], line 22\u001b[0m, in \u001b[0;36msample_next_state.<locals>.choice\u001b[0;34m(key, hsa)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39m@jax\u001b[39m\u001b[39m.\u001b[39mvmap\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mchoice\u001b[39m(key, hsa):\n\u001b[0;32m---> 22\u001b[0m     \u001b[39mreturn\u001b[39;00m jax\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(key, mdp\u001b[39m.\u001b[39;49mS_set, shape\u001b[39m=\u001b[39;49m(N,), p\u001b[39m=\u001b[39;49mP[hsa[\u001b[39m0\u001b[39;49m], hsa[\u001b[39m1\u001b[39;49m], hsa[\u001b[39m2\u001b[39;49m]])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/shumi-VTLwuKSy-py3.9/lib/python3.9/site-packages/jax/_src/random.py:526\u001b[0m, in \u001b[0;36mchoice\u001b[0;34m(key, a, shape, replace, p, axis)\u001b[0m\n\u001b[1;32m    524\u001b[0m p_arr, \u001b[39m=\u001b[39m _promote_dtypes_inexact(p)\n\u001b[1;32m    525\u001b[0m \u001b[39mif\u001b[39;00m p_arr\u001b[39m.\u001b[39mshape \u001b[39m!=\u001b[39m (n_inputs,):\n\u001b[0;32m--> 526\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mp must be None or match the shape of a\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    527\u001b[0m \u001b[39mif\u001b[39;00m replace:\n\u001b[1;32m    528\u001b[0m   p_cuml \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mcumsum(p_arr)\n",
      "\u001b[0;31mValueError\u001b[0m: p must be None or match the shape of a"
     ]
    }
   ],
   "source": [
    "from jax.random import PRNGKey\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"N\",))\n",
    "def sample_next_state(mdp: MDP, N: int, key: PRNGKey, D: jnp.array):\n",
    "    \"\"\" 遷移行列Pに従って次の状態をN個サンプルします\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        N (int): サンプルする個数\n",
    "        key (PRNGKey)\n",
    "        D (np.ndarray): 状態行動対の集合 [(h1, s1, a1), (h1, s2, a2), ...]\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        next_s_set (np.ndarray): (len(D) x N) の次状態の集合\n",
    "    \"\"\"\n",
    "    new_key, key = jax.random.split(key)\n",
    "    keys = jax.random.split(key, num=len(D))\n",
    "\n",
    "    @jax.vmap\n",
    "    def choice(key, hsa):\n",
    "        return jax.random.choice(key, mdp.S_set, shape=(N,), p=P[hsa[0], hsa[1], hsa[2]])\n",
    "\n",
    "    next_s = choice(keys, D)\n",
    "    return new_key, next_s\n",
    "\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "N = 20000\n",
    "D = jnp.array([(0, 1, 2), (1, 2, 1), (0, 0, 0), (4, 3, 1)])\n",
    "key, next_states = sample_next_state(mdp, N, key, D)\n",
    "assert next_states.shape == (len(D), N)\n",
    "h, s, a = D[0]\n",
    "P0_approx = jnp.bincount(next_states[0], minlength=S) / N\n",
    "np.testing.assert_allclose(P0_approx, mdp.P[h, s, a], atol=1e-2)\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnames=(\"H\",))\n",
    "def _collect_samples(mdp: MDP, H: int, key: PRNGKey, q: jnp.array, init_s: int):\n",
    "    \"\"\" エピソードの開始から終了まで、MDPとインタラクションしてサンプルをH個集めます。qの貪欲方策に従って動きます。\n",
    "    Args:\n",
    "        mdp (MDP)\n",
    "        H (int): ホライゾン\n",
    "        key (PRNGKey)\n",
    "        q (jnp.ndarray): 行動価値関数\n",
    "        init_s (int): 初期状態\n",
    "\n",
    "    Returns:\n",
    "        new_key (PRNGKey)\n",
    "        sars (jnp.ndarray): (状態, 行動, 報酬, 次状態) x H の軌跡\n",
    "    \"\"\"\n",
    "    chex.assert_shape(q, (H, mdp.S, mdp.A))\n",
    "\n",
    "    def body_fn(h, args):\n",
    "        key, sars, s = args\n",
    "        a = q[h, s].argmax()\n",
    "        r = mdp.rew[h, s, a]\n",
    "        D = jnp.array([(h, s, a),])\n",
    "        key, next_s = sample_next_state(mdp, 1, key, D)\n",
    "        next_s = next_s[0][0]\n",
    "        sars = sars.at[h].set((s, a, r, next_s))\n",
    "        return key, sars, next_s\n",
    "\n",
    "    sars = jnp.zeros((H, 4))\n",
    "    args = key, sars, init_s\n",
    "    key, sars, next_s = jax.lax.fori_loop(0, H, body_fn, args)\n",
    "    return key, sars, next_s\n",
    "\n",
    "collect_samples = lambda mdp, key, q, init_s: _collect_samples(mdp, mdp.H, key, q, init_s)\n",
    "\n",
    "key, sars, next_s = collect_samples(mdp, key, mdp.optimal_Q, 0)\n",
    "assert sars.shape == (mdp.H, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('shumi-VTLwuKSy-py3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6b7cac5e0d2ff733f340da4d53ae5ecfef7f7ad39623f5982b029a09306b36b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
